digraph {
	graph [size="155.1,155.1"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2295572754128 [label="
 (1, 128)" fillcolor=darkolivegreen1]
	2295603136832 [label=DivBackward0]
	2295603137312 -> 2295603136832
	2295603137312 [label=ReluBackward0]
	2295603137168 -> 2295603137312
	2295603137168 [label=NativeLayerNormBackward0]
	2295603137072 -> 2295603137168
	2295603137072 [label=AddmmBackward0]
	2295603136496 -> 2295603137072
	2295572744288 [label="base_model.fc.bias
 (128)" fillcolor=lightblue]
	2295572744288 -> 2295603136496
	2295603136496 [label=AccumulateGrad]
	2295603136544 -> 2295603137072
	2295603136544 [label=ViewBackward0]
	2295603136928 -> 2295603136544
	2295603136928 [label=MeanBackward1]
	2295603136688 -> 2295603136928
	2295603136688 [label=ReluBackward0]
	2295603136784 -> 2295603136688
	2295603136784 [label=AddBackward0]
	2295603136016 -> 2295603136784
	2295603136016 [label=CudnnBatchNormBackward0]
	2295603136160 -> 2295603136016
	2295603136160 [label=ConvolutionBackward0]
	2295603135920 -> 2295603136160
	2295603135920 [label=ReluBackward0]
	2295603135440 -> 2295603135920
	2295603135440 [label=CudnnBatchNormBackward0]
	2295603135104 -> 2295603135440
	2295603135104 [label=ConvolutionBackward0]
	2295603134960 -> 2295603135104
	2295603134960 [label=ReluBackward0]
	2295603135968 -> 2295603134960
	2295603135968 [label=CudnnBatchNormBackward0]
	2295603136256 -> 2295603135968
	2295603136256 [label=ConvolutionBackward0]
	2295603136592 -> 2295603136256
	2295603136592 [label=ReluBackward0]
	2295603135152 -> 2295603136592
	2295603135152 [label=AddBackward0]
	2295603134912 -> 2295603135152
	2295603134912 [label=CudnnBatchNormBackward0]
	2295603135632 -> 2295603134912
	2295603135632 [label=ConvolutionBackward0]
	2295603135824 -> 2295603135632
	2295603135824 [label=ReluBackward0]
	2295603135056 -> 2295603135824
	2295603135056 [label=CudnnBatchNormBackward0]
	2295575871296 -> 2295603135056
	2295575871296 [label=ConvolutionBackward0]
	2295575862512 -> 2295575871296
	2295575862512 [label=ReluBackward0]
	2295575867888 -> 2295575862512
	2295575867888 [label=CudnnBatchNormBackward0]
	2295575867264 -> 2295575867888
	2295575867264 [label=ConvolutionBackward0]
	2295603135488 -> 2295575867264
	2295603135488 [label=ReluBackward0]
	2295575862176 -> 2295603135488
	2295575862176 [label=AddBackward0]
	2295575858384 -> 2295575862176
	2295575858384 [label=CudnnBatchNormBackward0]
	2295575862656 -> 2295575858384
	2295575862656 [label=ConvolutionBackward0]
	2295575863184 -> 2295575862656
	2295575863184 [label=ReluBackward0]
	2295575861120 -> 2295575863184
	2295575861120 [label=CudnnBatchNormBackward0]
	2295575867120 -> 2295575861120
	2295575867120 [label=ConvolutionBackward0]
	2295575867504 -> 2295575867120
	2295575867504 [label=ReluBackward0]
	2295575857328 -> 2295575867504
	2295575857328 [label=CudnnBatchNormBackward0]
	2295575870144 -> 2295575857328
	2295575870144 [label=ConvolutionBackward0]
	2295575867168 -> 2295575870144
	2295575867168 [label=ReluBackward0]
	2295575862032 -> 2295575867168
	2295575862032 [label=AddBackward0]
	2295575867408 -> 2295575862032
	2295575867408 [label=CudnnBatchNormBackward0]
	2295603116352 -> 2295575867408
	2295603116352 [label=ConvolutionBackward0]
	2295603117120 -> 2295603116352
	2295603117120 [label=ReluBackward0]
	2295603115584 -> 2295603117120
	2295603115584 [label=CudnnBatchNormBackward0]
	2295603116688 -> 2295603115584
	2295603116688 [label=ConvolutionBackward0]
	2295603115392 -> 2295603116688
	2295603115392 [label=ReluBackward0]
	2295603116112 -> 2295603115392
	2295603116112 [label=CudnnBatchNormBackward0]
	2295566236016 -> 2295603116112
	2295566236016 [label=ConvolutionBackward0]
	2295603116544 -> 2295566236016
	2295603116544 [label=ReluBackward0]
	2295599006480 -> 2295603116544
	2295599006480 [label=AddBackward0]
	2295599006672 -> 2295599006480
	2295599006672 [label=CudnnBatchNormBackward0]
	2295599007728 -> 2295599006672
	2295599007728 [label=ConvolutionBackward0]
	2295573755792 -> 2295599007728
	2295573755792 [label=ReluBackward0]
	2295573743168 -> 2295573755792
	2295573743168 [label=CudnnBatchNormBackward0]
	2295573744128 -> 2295573743168
	2295573744128 [label=ConvolutionBackward0]
	2295573744080 -> 2295573744128
	2295573744080 [label=ReluBackward0]
	2295573749360 -> 2295573744080
	2295573749360 [label=CudnnBatchNormBackward0]
	2295573743120 -> 2295573749360
	2295573743120 [label=ConvolutionBackward0]
	2295599006336 -> 2295573743120
	2295599006336 [label=ReluBackward0]
	2295573749024 -> 2295599006336
	2295573749024 [label=AddBackward0]
	2295573743072 -> 2295573749024
	2295573743072 [label=CudnnBatchNormBackward0]
	2295573749168 -> 2295573743072
	2295573749168 [label=ConvolutionBackward0]
	2295573757712 -> 2295573749168
	2295573757712 [label=ReluBackward0]
	2295573748832 -> 2295573757712
	2295573748832 [label=CudnnBatchNormBackward0]
	2295573743312 -> 2295573748832
	2295573743312 [label=ConvolutionBackward0]
	2295572400592 -> 2295573743312
	2295572400592 [label=ReluBackward0]
	2295572400880 -> 2295572400592
	2295572400880 [label=CudnnBatchNormBackward0]
	2295572401408 -> 2295572400880
	2295572401408 [label=ConvolutionBackward0]
	2295573743840 -> 2295572401408
	2295573743840 [label=ReluBackward0]
	2295572401744 -> 2295573743840
	2295572401744 [label=AddBackward0]
	2295572401360 -> 2295572401744
	2295572401360 [label=CudnnBatchNormBackward0]
	2295572401216 -> 2295572401360
	2295572401216 [label=ConvolutionBackward0]
	2295572401072 -> 2295572401216
	2295572401072 [label=ReluBackward0]
	2295572401264 -> 2295572401072
	2295572401264 [label=CudnnBatchNormBackward0]
	2295572401792 -> 2295572401264
	2295572401792 [label=ConvolutionBackward0]
	2295572086992 -> 2295572401792
	2295572086992 [label=ReluBackward0]
	2295572088240 -> 2295572086992
	2295572088240 [label=CudnnBatchNormBackward0]
	2295572087376 -> 2295572088240
	2295572087376 [label=ConvolutionBackward0]
	2295572401696 -> 2295572087376
	2295572401696 [label=ReluBackward0]
	2295572087712 -> 2295572401696
	2295572087712 [label=AddBackward0]
	2295531786912 -> 2295572087712
	2295531786912 [label=CudnnBatchNormBackward0]
	2295531789408 -> 2295531786912
	2295531789408 [label=ConvolutionBackward0]
	2295531789600 -> 2295531789408
	2295531789600 [label=ReluBackward0]
	2295531790512 -> 2295531789600
	2295531790512 [label=CudnnBatchNormBackward0]
	2295531793200 -> 2295531790512
	2295531793200 [label=ConvolutionBackward0]
	2295531790608 -> 2295531793200
	2295531790608 [label=ReluBackward0]
	2295531791472 -> 2295531790608
	2295531791472 [label=CudnnBatchNormBackward0]
	2295531790896 -> 2295531791472
	2295531790896 [label=ConvolutionBackward0]
	2295531786528 -> 2295531790896
	2295531786528 [label=ReluBackward0]
	2295531786336 -> 2295531786528
	2295531786336 [label=AddBackward0]
	2295531787584 -> 2295531786336
	2295531787584 [label=CudnnBatchNormBackward0]
	2295531787344 -> 2295531787584
	2295531787344 [label=ConvolutionBackward0]
	2295531790656 -> 2295531787344
	2295531790656 [label=ReluBackward0]
	2295531788112 -> 2295531790656
	2295531788112 [label=CudnnBatchNormBackward0]
	2295531791184 -> 2295531788112
	2295531791184 [label=ConvolutionBackward0]
	2295531793056 -> 2295531791184
	2295531793056 [label=ReluBackward0]
	2295531792048 -> 2295531793056
	2295531792048 [label=CudnnBatchNormBackward0]
	2295531791376 -> 2295531792048
	2295531791376 [label=ConvolutionBackward0]
	2295531788928 -> 2295531791376
	2295531788928 [label=ReluBackward0]
	2295531793344 -> 2295531788928
	2295531793344 [label=AddBackward0]
	2295531793584 -> 2295531793344
	2295531793584 [label=CudnnBatchNormBackward0]
	2295531786624 -> 2295531793584
	2295531786624 [label=ConvolutionBackward0]
	2295531795936 -> 2295531786624
	2295531795936 [label=ReluBackward0]
	2295531792912 -> 2295531795936
	2295531792912 [label=CudnnBatchNormBackward0]
	2295531786960 -> 2295531792912
	2295531786960 [label=ConvolutionBackward0]
	2295531787296 -> 2295531786960
	2295531787296 [label=ReluBackward0]
	2295531789936 -> 2295531787296
	2295531789936 [label=CudnnBatchNormBackward0]
	2295531788592 -> 2295531789936
	2295531788592 [label=ConvolutionBackward0]
	2295531789216 -> 2295531788592
	2295531789216 [label=ReluBackward0]
	2295572032640 -> 2295531789216
	2295572032640 [label=AddBackward0]
	2295572035232 -> 2295572032640
	2295572035232 [label=CudnnBatchNormBackward0]
	2295572028176 -> 2295572035232
	2295572028176 [label=ConvolutionBackward0]
	2295572031968 -> 2295572028176
	2295572031968 [label=ReluBackward0]
	2295572034944 -> 2295572031968
	2295572034944 [label=CudnnBatchNormBackward0]
	2295572031536 -> 2295572034944
	2295572031536 [label=ConvolutionBackward0]
	2295572032064 -> 2295572031536
	2295572032064 [label=ReluBackward0]
	2295572030384 -> 2295572032064
	2295572030384 [label=CudnnBatchNormBackward0]
	2295572035136 -> 2295572030384
	2295572035136 [label=ConvolutionBackward0]
	2295572031248 -> 2295572035136
	2295572031248 [label=ReluBackward0]
	2295572034992 -> 2295572031248
	2295572034992 [label=AddBackward0]
	2295572034752 -> 2295572034992
	2295572034752 [label=CudnnBatchNormBackward0]
	2295572033888 -> 2295572034752
	2295572033888 [label=ConvolutionBackward0]
	2295572032976 -> 2295572033888
	2295572032976 [label=ReluBackward0]
	2295572032880 -> 2295572032976
	2295572032880 [label=CudnnBatchNormBackward0]
	2295572032928 -> 2295572032880
	2295572032928 [label=ConvolutionBackward0]
	2295572028416 -> 2295572032928
	2295572028416 [label=ReluBackward0]
	2295572029040 -> 2295572028416
	2295572029040 [label=CudnnBatchNormBackward0]
	2295572028080 -> 2295572029040
	2295572028080 [label=ConvolutionBackward0]
	2295572029136 -> 2295572028080
	2295572029136 [label=ReluBackward0]
	2295572029280 -> 2295572029136
	2295572029280 [label=AddBackward0]
	2295572029520 -> 2295572029280
	2295572029520 [label=CudnnBatchNormBackward0]
	2295572030048 -> 2295572029520
	2295572030048 [label=ConvolutionBackward0]
	2295572029568 -> 2295572030048
	2295572029568 [label=ReluBackward0]
	2295572035568 -> 2295572029568
	2295572035568 [label=CudnnBatchNormBackward0]
	2295572031680 -> 2295572035568
	2295572031680 [label=ConvolutionBackward0]
	2295572033216 -> 2295572031680
	2295572033216 [label=ReluBackward0]
	2295572035808 -> 2295572033216
	2295572035808 [label=CudnnBatchNormBackward0]
	2295572028464 -> 2295572035808
	2295572028464 [label=ConvolutionBackward0]
	2295572027312 -> 2295572028464
	2295572027312 [label=ReluBackward0]
	2295572033744 -> 2295572027312
	2295572033744 [label=AddBackward0]
	2295572028608 -> 2295572033744
	2295572028608 [label=CudnnBatchNormBackward0]
	2295572034560 -> 2295572028608
	2295572034560 [label=ConvolutionBackward0]
	2295572034224 -> 2295572034560
	2295572034224 [label=ReluBackward0]
	2295572036960 -> 2295572034224
	2295572036960 [label=CudnnBatchNormBackward0]
	2295572036912 -> 2295572036960
	2295572036912 [label=ConvolutionBackward0]
	2295572037152 -> 2295572036912
	2295572037152 [label=ReluBackward0]
	2295572036816 -> 2295572037152
	2295572036816 [label=CudnnBatchNormBackward0]
	2295572036432 -> 2295572036816
	2295572036432 [label=ConvolutionBackward0]
	2295572029952 -> 2295572036432
	2295572029952 [label=ReluBackward0]
	2295572036048 -> 2295572029952
	2295572036048 [label=AddBackward0]
	2295572036384 -> 2295572036048
	2295572036384 [label=CudnnBatchNormBackward0]
	2295572036144 -> 2295572036384
	2295572036144 [label=ConvolutionBackward0]
	2295572035952 -> 2295572036144
	2295572035952 [label=ReluBackward0]
	2295572036000 -> 2295572035952
	2295572036000 [label=CudnnBatchNormBackward0]
	2295572035712 -> 2295572036000
	2295572035712 [label=ConvolutionBackward0]
	2295572032304 -> 2295572035712
	2295572032304 [label=ReluBackward0]
	2295572031488 -> 2295572032304
	2295572031488 [label=CudnnBatchNormBackward0]
	2295572032784 -> 2295572031488
	2295572032784 [label=ConvolutionBackward0]
	2295572036624 -> 2295572032784
	2295572036624 [label=ReluBackward0]
	2295572031440 -> 2295572036624
	2295572031440 [label=AddBackward0]
	2295572030336 -> 2295572031440
	2295572030336 [label=CudnnBatchNormBackward0]
	2295572029664 -> 2295572030336
	2295572029664 [label=ConvolutionBackward0]
	2295572033360 -> 2295572029664
	2295572033360 [label=ReluBackward0]
	2295572032832 -> 2295572033360
	2295572032832 [label=CudnnBatchNormBackward0]
	2295572034272 -> 2295572032832
	2295572034272 [label=ConvolutionBackward0]
	2295572028656 -> 2295572034272
	2295572028656 [label=ReluBackward0]
	2295572028032 -> 2295572028656
	2295572028032 [label=CudnnBatchNormBackward0]
	2295572028992 -> 2295572028032
	2295572028992 [label=ConvolutionBackward0]
	2295572028368 -> 2295572028992
	2295572028368 [label=MaxPool2DWithIndicesBackward0]
	2295572029088 -> 2295572028368
	2295572029088 [label=ReluBackward0]
	2295572028800 -> 2295572029088
	2295572028800 [label=CudnnBatchNormBackward0]
	2295572027696 -> 2295572028800
	2295572027696 [label=ConvolutionBackward0]
	2295572026544 -> 2295572027696
	2295606460560 [label="base_model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2295606460560 -> 2295572026544
	2295572026544 [label=AccumulateGrad]
	2295572028752 -> 2295572028800
	2295606462400 [label="base_model.bn1.weight
 (64)" fillcolor=lightblue]
	2295606462400 -> 2295572028752
	2295572028752 [label=AccumulateGrad]
	2295572028128 -> 2295572028800
	2295573240128 [label="base_model.bn1.bias
 (64)" fillcolor=lightblue]
	2295573240128 -> 2295572028128
	2295572028128 [label=AccumulateGrad]
	2295572029712 -> 2295572028992
	2295573244128 [label="base_model.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2295573244128 -> 2295572029712
	2295572029712 [label=AccumulateGrad]
	2295572029232 -> 2295572028032
	2295573244608 [label="base_model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2295573244608 -> 2295572029232
	2295572029232 [label=AccumulateGrad]
	2295572028704 -> 2295572028032
	2295573234288 [label="base_model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2295573234288 -> 2295572028704
	2295572028704 [label=AccumulateGrad]
	2295572027840 -> 2295572034272
	2295573245648 [label="base_model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2295573245648 -> 2295572027840
	2295572027840 [label=AccumulateGrad]
	2295572034368 -> 2295572032832
	2295573245728 [label="base_model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2295573245728 -> 2295572034368
	2295572034368 [label=AccumulateGrad]
	2295572028320 -> 2295572032832
	2295573245888 [label="base_model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2295573245888 -> 2295572028320
	2295572028320 [label=AccumulateGrad]
	2295572033984 -> 2295572029664
	2295573246608 [label="base_model.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2295573246608 -> 2295572033984
	2295572033984 [label=AccumulateGrad]
	2295572029856 -> 2295572030336
	2295573248048 [label="base_model.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2295573248048 -> 2295572029856
	2295572029856 [label=AccumulateGrad]
	2295572033072 -> 2295572030336
	2295573248128 [label="base_model.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2295573248128 -> 2295572033072
	2295572033072 [label=AccumulateGrad]
	2295572030720 -> 2295572031440
	2295572030720 [label=CudnnBatchNormBackward0]
	2295572027072 -> 2295572030720
	2295572027072 [label=ConvolutionBackward0]
	2295572028368 -> 2295572027072
	2295572030288 -> 2295572027072
	2295573237168 [label="base_model.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2295573237168 -> 2295572030288
	2295572030288 [label=AccumulateGrad]
	2295572034656 -> 2295572030720
	2295573240688 [label="base_model.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2295573240688 -> 2295572034656
	2295572034656 [label=AccumulateGrad]
	2295572034032 -> 2295572030720
	2295573240608 [label="base_model.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2295573240608 -> 2295572034032
	2295572034032 [label=AccumulateGrad]
	2295572027792 -> 2295572032784
	2295573248448 [label="base_model.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2295573248448 -> 2295572027792
	2295572027792 [label=AccumulateGrad]
	2295572031824 -> 2295572031488
	2295573248528 [label="base_model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2295573248528 -> 2295572031824
	2295572031824 [label=AccumulateGrad]
	2295572031776 -> 2295572031488
	2295573248848 [label="base_model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2295573248848 -> 2295572031776
	2295572031776 [label=AccumulateGrad]
	2295572029184 -> 2295572035712
	2295573247648 [label="base_model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2295573247648 -> 2295572029184
	2295572029184 [label=AccumulateGrad]
	2295572035904 -> 2295572036000
	2295573247488 [label="base_model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2295573247488 -> 2295572035904
	2295572035904 [label=AccumulateGrad]
	2295572032400 -> 2295572036000
	2295573249808 [label="base_model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2295573249808 -> 2295572032400
	2295572032400 [label=AccumulateGrad]
	2295572035856 -> 2295572036144
	2295573241008 [label="base_model.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2295573241008 -> 2295572035856
	2295572035856 [label=AccumulateGrad]
	2295572036192 -> 2295572036384
	2295573237328 [label="base_model.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2295573237328 -> 2295572036192
	2295572036192 [label=AccumulateGrad]
	2295572036240 -> 2295572036384
	2295573241328 [label="base_model.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2295573241328 -> 2295572036240
	2295572036240 [label=AccumulateGrad]
	2295572036624 -> 2295572036048
	2295572036528 -> 2295572036432
	2295573241888 [label="base_model.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2295573241888 -> 2295572036528
	2295572036528 [label=AccumulateGrad]
	2295572037104 -> 2295572036816
	2295573241648 [label="base_model.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2295573241648 -> 2295572037104
	2295572037104 [label=AccumulateGrad]
	2295572036720 -> 2295572036816
	2295573241408 [label="base_model.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2295573241408 -> 2295572036720
	2295572036720 [label=AccumulateGrad]
	2295572037200 -> 2295572036912
	2295573243248 [label="base_model.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2295573243248 -> 2295572037200
	2295572037200 [label=AccumulateGrad]
	2295572034464 -> 2295572036960
	2295573241568 [label="base_model.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2295573241568 -> 2295572034464
	2295572034464 [label=AccumulateGrad]
	2295572031872 -> 2295572036960
	2295573243168 [label="base_model.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2295573243168 -> 2295572031872
	2295572031872 [label=AccumulateGrad]
	2295572034416 -> 2295572034560
	2295606470000 [label="base_model.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2295606470000 -> 2295572034416
	2295572034416 [label=AccumulateGrad]
	2295572033504 -> 2295572028608
	2295573233888 [label="base_model.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2295573233888 -> 2295572033504
	2295572033504 [label=AccumulateGrad]
	2295572033840 -> 2295572028608
	2295573242848 [label="base_model.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2295573242848 -> 2295572033840
	2295572033840 [label=AccumulateGrad]
	2295572029952 -> 2295572033744
	2295572034080 -> 2295572028464
	2295573247728 [label="base_model.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2295573247728 -> 2295572034080
	2295572034080 [label=AccumulateGrad]
	2295572030192 -> 2295572035808
	2295573249648 [label="base_model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2295573249648 -> 2295572030192
	2295572030192 [label=AccumulateGrad]
	2295572031920 -> 2295572035808
	2295573249248 [label="base_model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2295573249248 -> 2295572031920
	2295572031920 [label=AccumulateGrad]
	2295572033552 -> 2295572031680
	2295573246208 [label="base_model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2295573246208 -> 2295572033552
	2295572033552 [label=AccumulateGrad]
	2295572030144 -> 2295572035568
	2295573246688 [label="base_model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2295573246688 -> 2295572030144
	2295572030144 [label=AccumulateGrad]
	2295572033408 -> 2295572035568
	2295573245568 [label="base_model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2295573245568 -> 2295572033408
	2295572033408 [label=AccumulateGrad]
	2295572028944 -> 2295572030048
	2295573244048 [label="base_model.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2295573244048 -> 2295572028944
	2295572028944 [label=AccumulateGrad]
	2295572030912 -> 2295572029520
	2295573237408 [label="base_model.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2295573237408 -> 2295572030912
	2295572030912 [label=AccumulateGrad]
	2295572029904 -> 2295572029520
	2295573240448 [label="base_model.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2295573240448 -> 2295572029904
	2295572029904 [label=AccumulateGrad]
	2295572031584 -> 2295572029280
	2295572031584 [label=CudnnBatchNormBackward0]
	2295572033936 -> 2295572031584
	2295572033936 [label=ConvolutionBackward0]
	2295572027312 -> 2295572033936
	2295572030480 -> 2295572033936
	2295573243488 [label="base_model.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2295573243488 -> 2295572030480
	2295572030480 [label=AccumulateGrad]
	2295572031296 -> 2295572031584
	2295573242528 [label="base_model.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2295573242528 -> 2295572031296
	2295572031296 [label=AccumulateGrad]
	2295572029472 -> 2295572031584
	2295573242448 [label="base_model.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2295573242448 -> 2295572029472
	2295572029472 [label=AccumulateGrad]
	2295572031056 -> 2295572028080
	2295573239168 [label="base_model.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2295573239168 -> 2295572031056
	2295572031056 [label=AccumulateGrad]
	2295572027216 -> 2295572029040
	2295573238768 [label="base_model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2295573238768 -> 2295572027216
	2295572027216 [label=AccumulateGrad]
	2295572030432 -> 2295572029040
	2295573238208 [label="base_model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2295573238208 -> 2295572030432
	2295572030432 [label=AccumulateGrad]
	2295572032448 -> 2295572032928
	2295573207520 [label="base_model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2295573207520 -> 2295572032448
	2295572032448 [label=AccumulateGrad]
	2295572033648 -> 2295572032880
	2295573213360 [label="base_model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2295573213360 -> 2295572033648
	2295572033648 [label=AccumulateGrad]
	2295572033696 -> 2295572032880
	2295573202880 [label="base_model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2295573202880 -> 2295572033696
	2295572033696 [label=AccumulateGrad]
	2295572032496 -> 2295572033888
	2295573213920 [label="base_model.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2295573213920 -> 2295572032496
	2295572032496 [label=AccumulateGrad]
	2295572033168 -> 2295572034752
	2295573214240 [label="base_model.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2295573214240 -> 2295572033168
	2295572033168 [label=AccumulateGrad]
	2295572034848 -> 2295572034752
	2295573213120 [label="base_model.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2295573213120 -> 2295572034848
	2295572034848 [label=AccumulateGrad]
	2295572029136 -> 2295572034992
	2295572034320 -> 2295572035136
	2295573212880 [label="base_model.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2295573212880 -> 2295572034320
	2295572034320 [label=AccumulateGrad]
	2295572032688 -> 2295572030384
	2295573213760 [label="base_model.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2295573213760 -> 2295572032688
	2295572032688 [label=AccumulateGrad]
	2295572037536 -> 2295572030384
	2295573213600 [label="base_model.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2295573213600 -> 2295572037536
	2295572037536 [label=AccumulateGrad]
	2295572032160 -> 2295572031536
	2295573216640 [label="base_model.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2295573216640 -> 2295572032160
	2295572032160 [label=AccumulateGrad]
	2295572032112 -> 2295572034944
	2295573212800 [label="base_model.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2295573212800 -> 2295572032112
	2295572032112 [label=AccumulateGrad]
	2295572031008 -> 2295572034944
	2295573212400 [label="base_model.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2295573212400 -> 2295572031008
	2295572031008 [label=AccumulateGrad]
	2295572035184 -> 2295572028176
	2295573204480 [label="base_model.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2295573204480 -> 2295572035184
	2295572035184 [label=AccumulateGrad]
	2295572031728 -> 2295572035232
	2295573205520 [label="base_model.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2295573205520 -> 2295572031728
	2295572031728 [label=AccumulateGrad]
	2295572027888 -> 2295572035232
	2295573212000 [label="base_model.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2295573212000 -> 2295572027888
	2295572027888 [label=AccumulateGrad]
	2295572031248 -> 2295572032640
	2295531786720 -> 2295531788592
	2295573211760 [label="base_model.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2295573211760 -> 2295531786720
	2295531786720 [label=AccumulateGrad]
	2295531792192 -> 2295531789936
	2295573210880 [label="base_model.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2295573210880 -> 2295531792192
	2295531792192 [label=AccumulateGrad]
	2295531787056 -> 2295531789936
	2295573202720 [label="base_model.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2295573202720 -> 2295531787056
	2295531787056 [label=AccumulateGrad]
	2295531787536 -> 2295531786960
	2295573203360 [label="base_model.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2295573203360 -> 2295531787536
	2295531787536 [label=AccumulateGrad]
	2295531794640 -> 2295531792912
	2295573211360 [label="base_model.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2295573211360 -> 2295531794640
	2295531794640 [label=AccumulateGrad]
	2295531794736 -> 2295531792912
	2295573211840 [label="base_model.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2295573211840 -> 2295531794736
	2295531794736 [label=AccumulateGrad]
	2295531795648 -> 2295531786624
	2295573214160 [label="base_model.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2295573214160 -> 2295531795648
	2295531795648 [label=AccumulateGrad]
	2295531794016 -> 2295531793584
	2295573210960 [label="base_model.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2295573210960 -> 2295531794016
	2295531794016 [label=AccumulateGrad]
	2295531791280 -> 2295531793584
	2295573215600 [label="base_model.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2295573215600 -> 2295531791280
	2295531791280 [label=AccumulateGrad]
	2295531789216 -> 2295531793344
	2295531788448 -> 2295531791376
	2295573217040 [label="base_model.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2295573217040 -> 2295531788448
	2295531788448 [label=AccumulateGrad]
	2295531792384 -> 2295531792048
	2295573201200 [label="base_model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2295573201200 -> 2295531792384
	2295531792384 [label=AccumulateGrad]
	2295531789744 -> 2295531792048
	2295573201280 [label="base_model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2295573201280 -> 2295531789744
	2295531789744 [label=AccumulateGrad]
	2295531793392 -> 2295531791184
	2295573202640 [label="base_model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2295573202640 -> 2295531793392
	2295531793392 [label=AccumulateGrad]
	2295531788256 -> 2295531788112
	2295573205360 [label="base_model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2295573205360 -> 2295531788256
	2295531788256 [label=AccumulateGrad]
	2295531790848 -> 2295531788112
	2295573202560 [label="base_model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2295573202560 -> 2295531790848
	2295531790848 [label=AccumulateGrad]
	2295531796368 -> 2295531787344
	2295573203920 [label="base_model.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2295573203920 -> 2295531796368
	2295531796368 [label=AccumulateGrad]
	2295531786576 -> 2295531787584
	2295573204800 [label="base_model.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2295573204800 -> 2295531786576
	2295531786576 [label=AccumulateGrad]
	2295531787872 -> 2295531787584
	2295573204880 [label="base_model.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2295573204880 -> 2295531787872
	2295531787872 [label=AccumulateGrad]
	2295531788640 -> 2295531786336
	2295531788640 [label=CudnnBatchNormBackward0]
	2295531793536 -> 2295531788640
	2295531793536 [label=ConvolutionBackward0]
	2295531788928 -> 2295531793536
	2295531791088 -> 2295531793536
	2295573201840 [label="base_model.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2295573201840 -> 2295531791088
	2295531791088 [label=AccumulateGrad]
	2295531791760 -> 2295531788640
	2295573216720 [label="base_model.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2295573216720 -> 2295531791760
	2295531791760 [label=AccumulateGrad]
	2295531788496 -> 2295531788640
	2295573214640 [label="base_model.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2295573214640 -> 2295531788496
	2295531788496 [label=AccumulateGrad]
	2295531789072 -> 2295531790896
	2295573205600 [label="base_model.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2295573205600 -> 2295531789072
	2295531789072 [label=AccumulateGrad]
	2295531791328 -> 2295531791472
	2295573216240 [label="base_model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2295573216240 -> 2295531791328
	2295531791328 [label=AccumulateGrad]
	2295531786096 -> 2295531791472
	2295573215840 [label="base_model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2295573215840 -> 2295531786096
	2295531786096 [label=AccumulateGrad]
	2295531793632 -> 2295531793200
	2295573207040 [label="base_model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2295573207040 -> 2295531793632
	2295531793632 [label=AccumulateGrad]
	2295531787680 -> 2295531790512
	2295573207200 [label="base_model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2295573207200 -> 2295531787680
	2295531787680 [label=AccumulateGrad]
	2295531789312 -> 2295531790512
	2295573206720 [label="base_model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2295573206720 -> 2295531789312
	2295531789312 [label=AccumulateGrad]
	2295531787824 -> 2295531789408
	2295573207280 [label="base_model.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2295573207280 -> 2295531787824
	2295531787824 [label=AccumulateGrad]
	2295531786768 -> 2295531786912
	2295573215920 [label="base_model.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2295573215920 -> 2295531786768
	2295531786768 [label=AccumulateGrad]
	2295531787104 -> 2295531786912
	2295573207760 [label="base_model.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2295573207760 -> 2295531787104
	2295531787104 [label=AccumulateGrad]
	2295531786528 -> 2295572087712
	2295572087808 -> 2295572087376
	2295573208720 [label="base_model.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2295573208720 -> 2295572087808
	2295572087808 [label=AccumulateGrad]
	2295572086848 -> 2295572088240
	2295573208640 [label="base_model.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2295573208640 -> 2295572086848
	2295572086848 [label=AccumulateGrad]
	2295572087040 -> 2295572088240
	2295573216080 [label="base_model.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2295573216080 -> 2295572087040
	2295572087040 [label=AccumulateGrad]
	2295572088000 -> 2295572401792
	2295573209120 [label="base_model.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2295573209120 -> 2295572088000
	2295572088000 [label=AccumulateGrad]
	2295572401312 -> 2295572401264
	2295573209280 [label="base_model.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2295573209280 -> 2295572401312
	2295572401312 [label=AccumulateGrad]
	2295572400928 -> 2295572401264
	2295573210080 [label="base_model.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2295573210080 -> 2295572400928
	2295572400928 [label=AccumulateGrad]
	2295572401024 -> 2295572401216
	2295573211280 [label="base_model.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2295573211280 -> 2295572401024
	2295572401024 [label=AccumulateGrad]
	2295572401648 -> 2295572401360
	2295573207600 [label="base_model.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2295573207600 -> 2295572401648
	2295572401648 [label=AccumulateGrad]
	2295572400688 -> 2295572401360
	2295573208000 [label="base_model.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2295573208000 -> 2295572400688
	2295572400688 [label=AccumulateGrad]
	2295572401696 -> 2295572401744
	2295572401456 -> 2295572401408
	2295573209600 [label="base_model.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2295573209600 -> 2295572401456
	2295572401456 [label=AccumulateGrad]
	2295572401168 -> 2295572400880
	2295573209040 [label="base_model.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2295573209040 -> 2295572401168
	2295572401168 [label=AccumulateGrad]
	2295572401600 -> 2295572400880
	2295573208800 [label="base_model.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2295573208800 -> 2295572401600
	2295572401600 [label=AccumulateGrad]
	2295572401504 -> 2295573743312
	2295573206640 [label="base_model.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2295573206640 -> 2295572401504
	2295572401504 [label=AccumulateGrad]
	2295573749120 -> 2295573748832
	2295573208160 [label="base_model.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2295573208160 -> 2295573749120
	2295573749120 [label=AccumulateGrad]
	2295573748640 -> 2295573748832
	2295573207360 [label="base_model.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2295573207360 -> 2295573748640
	2295573748640 [label=AccumulateGrad]
	2295573757280 -> 2295573749168
	2295573204640 [label="base_model.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2295573204640 -> 2295573757280
	2295573757280 [label=AccumulateGrad]
	2295573743456 -> 2295573743072
	2295573211520 [label="base_model.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2295573211520 -> 2295573743456
	2295573743456 [label=AccumulateGrad]
	2295573743552 -> 2295573743072
	2295573204080 [label="base_model.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2295573204080 -> 2295573743552
	2295573743552 [label=AccumulateGrad]
	2295573743840 -> 2295573749024
	2295573743600 -> 2295573743120
	2295573203680 [label="base_model.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2295573203680 -> 2295573743600
	2295573743600 [label=AccumulateGrad]
	2295573749072 -> 2295573749360
	2295573216960 [label="base_model.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2295573216960 -> 2295573749072
	2295573749072 [label=AccumulateGrad]
	2295573749216 -> 2295573749360
	2295573202480 [label="base_model.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2295573202480 -> 2295573749216
	2295573749216 [label=AccumulateGrad]
	2295573749312 -> 2295573744128
	2295573212160 [label="base_model.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2295573212160 -> 2295573749312
	2295573749312 [label=AccumulateGrad]
	2295573744176 -> 2295573743168
	2295573216480 [label="base_model.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2295573216480 -> 2295573744176
	2295573744176 [label=AccumulateGrad]
	2295573744368 -> 2295573743168
	2295573211920 [label="base_model.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2295573211920 -> 2295573744368
	2295573744368 [label=AccumulateGrad]
	2295573756032 -> 2295599007728
	2295573210160 [label="base_model.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2295573210160 -> 2295573756032
	2295573756032 [label=AccumulateGrad]
	2295599007056 -> 2295599006672
	2295573212320 [label="base_model.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2295573212320 -> 2295599007056
	2295599007056 [label=AccumulateGrad]
	2295599007440 -> 2295599006672
	2295573211200 [label="base_model.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2295573211200 -> 2295599007440
	2295599007440 [label=AccumulateGrad]
	2295599006336 -> 2295599006480
	2295599008880 -> 2295566236016
	2295572749488 [label="base_model.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2295572749488 -> 2295599008880
	2295599008880 [label=AccumulateGrad]
	2295566236592 -> 2295603116112
	2295572749648 [label="base_model.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2295572749648 -> 2295566236592
	2295566236592 [label=AccumulateGrad]
	2295599007584 -> 2295603116112
	2295572752848 [label="base_model.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2295572752848 -> 2295599007584
	2295599007584 [label=AccumulateGrad]
	2295603117696 -> 2295603116688
	2295572742688 [label="base_model.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2295572742688 -> 2295603117696
	2295603117696 [label=AccumulateGrad]
	2295603115008 -> 2295603115584
	2295572742608 [label="base_model.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2295572742608 -> 2295603115008
	2295603115008 [label=AccumulateGrad]
	2295603116496 -> 2295603115584
	2295572742768 [label="base_model.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2295572742768 -> 2295603116496
	2295603116496 [label=AccumulateGrad]
	2295603117504 -> 2295603116352
	2295572745248 [label="base_model.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2295572745248 -> 2295603117504
	2295603117504 [label=AccumulateGrad]
	2295603112704 -> 2295575867408
	2295572747008 [label="base_model.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2295572747008 -> 2295603112704
	2295603112704 [label=AccumulateGrad]
	2295603117072 -> 2295575867408
	2295572747168 [label="base_model.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2295572747168 -> 2295603117072
	2295603117072 [label=AccumulateGrad]
	2295603116544 -> 2295575862032
	2295575860064 -> 2295575870144
	2295572749728 [label="base_model.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2295572749728 -> 2295575860064
	2295575860064 [label=AccumulateGrad]
	2295575860688 -> 2295575857328
	2295572749968 [label="base_model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2295572749968 -> 2295575860688
	2295575860688 [label=AccumulateGrad]
	2295575862560 -> 2295575857328
	2295572747568 [label="base_model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2295572747568 -> 2295575862560
	2295575862560 [label=AccumulateGrad]
	2295575861792 -> 2295575867120
	2295572757008 [label="base_model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2295572757008 -> 2295575861792
	2295575861792 [label=AccumulateGrad]
	2295575862752 -> 2295575861120
	2295572757728 [label="base_model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2295572757728 -> 2295575862752
	2295575862752 [label=AccumulateGrad]
	2295575869568 -> 2295575861120
	2295572755728 [label="base_model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2295572755728 -> 2295575869568
	2295575869568 [label=AccumulateGrad]
	2295575859056 -> 2295575862656
	2295572755968 [label="base_model.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2295572755968 -> 2295575859056
	2295575859056 [label=AccumulateGrad]
	2295575861888 -> 2295575858384
	2295572757168 [label="base_model.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2295572757168 -> 2295575861888
	2295575861888 [label=AccumulateGrad]
	2295575857616 -> 2295575858384
	2295572755808 [label="base_model.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2295572755808 -> 2295575857616
	2295575857616 [label=AccumulateGrad]
	2295575856992 -> 2295575862176
	2295575856992 [label=CudnnBatchNormBackward0]
	2295575870096 -> 2295575856992
	2295575870096 [label=ConvolutionBackward0]
	2295575867168 -> 2295575870096
	2295575867456 -> 2295575870096
	2295572753488 [label="base_model.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2295572753488 -> 2295575867456
	2295575867456 [label=AccumulateGrad]
	2295575867360 -> 2295575856992
	2295572753648 [label="base_model.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2295572753648 -> 2295575867360
	2295575867360 [label=AccumulateGrad]
	2295575868128 -> 2295575856992
	2295572753888 [label="base_model.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2295572753888 -> 2295575868128
	2295575868128 [label=AccumulateGrad]
	2295575866976 -> 2295575867264
	2295572749088 [label="base_model.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2295572749088 -> 2295575866976
	2295575866976 [label=AccumulateGrad]
	2295575870192 -> 2295575867888
	2295572754608 [label="base_model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2295572754608 -> 2295575870192
	2295575870192 [label=AccumulateGrad]
	2295575862128 -> 2295575867888
	2295572757408 [label="base_model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2295572757408 -> 2295575862128
	2295575862128 [label=AccumulateGrad]
	2295575861168 -> 2295575871296
	2295572750768 [label="base_model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2295572750768 -> 2295575861168
	2295575861168 [label=AccumulateGrad]
	2295575870528 -> 2295603135056
	2295572750688 [label="base_model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2295572750688 -> 2295575870528
	2295575870528 [label=AccumulateGrad]
	2295575868992 -> 2295603135056
	2295572753168 [label="base_model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2295572753168 -> 2295575868992
	2295575868992 [label=AccumulateGrad]
	2295603135344 -> 2295603135632
	2295572743088 [label="base_model.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2295572743088 -> 2295603135344
	2295603135344 [label=AccumulateGrad]
	2295603135008 -> 2295603134912
	2295572743408 [label="base_model.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2295572743408 -> 2295603135008
	2295603135008 [label=AccumulateGrad]
	2295603135584 -> 2295603134912
	2295572743488 [label="base_model.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2295572743488 -> 2295603135584
	2295603135584 [label=AccumulateGrad]
	2295603135488 -> 2295603135152
	2295603134864 -> 2295603136256
	2295572744608 [label="base_model.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2295572744608 -> 2295603134864
	2295603134864 [label=AccumulateGrad]
	2295603135872 -> 2295603135968
	2295572745328 [label="base_model.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2295572745328 -> 2295603135872
	2295603135872 [label=AccumulateGrad]
	2295603134672 -> 2295603135968
	2295572745648 [label="base_model.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2295572745648 -> 2295603134672
	2295603134672 [label=AccumulateGrad]
	2295603134528 -> 2295603135104
	2295572746688 [label="base_model.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2295572746688 -> 2295603134528
	2295603134528 [label=AccumulateGrad]
	2295603134816 -> 2295603135440
	2295572746528 [label="base_model.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2295572746528 -> 2295603134816
	2295603134816 [label=AccumulateGrad]
	2295603134768 -> 2295603135440
	2295572746608 [label="base_model.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2295572746608 -> 2295603134768
	2295603134768 [label=AccumulateGrad]
	2295603136112 -> 2295603136160
	2295572749808 [label="base_model.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2295572749808 -> 2295603136112
	2295603136112 [label=AccumulateGrad]
	2295603136208 -> 2295603136016
	2295572749888 [label="base_model.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2295572749888 -> 2295603136208
	2295603136208 [label=AccumulateGrad]
	2295603136304 -> 2295603136016
	2295572754048 [label="base_model.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2295572754048 -> 2295603136304
	2295603136304 [label=AccumulateGrad]
	2295603136592 -> 2295603136784
	2295603136448 -> 2295603137072
	2295603136448 [label=TBackward0]
	2295603136736 -> 2295603136448
	2295572758368 [label="base_model.fc.weight
 (128, 2048)" fillcolor=lightblue]
	2295572758368 -> 2295603136736
	2295603136736 [label=AccumulateGrad]
	2295603137120 -> 2295603137168
	2295572758208 [label="embedding_layer.0.weight
 (128)" fillcolor=lightblue]
	2295572758208 -> 2295603137120
	2295603137120 [label=AccumulateGrad]
	2295603137264 -> 2295603137168
	2295572757968 [label="embedding_layer.0.bias
 (128)" fillcolor=lightblue]
	2295572757968 -> 2295603137264
	2295603137264 [label=AccumulateGrad]
	2295603137456 -> 2295603136832
	2295603137456 [label=ExpandBackward0]
	2295603136400 -> 2295603137456
	2295603136400 [label=ClampMinBackward0]
	2295603136064 -> 2295603136400
	2295603136064 [label=LinalgVectorNormBackward0]
	2295603137312 -> 2295603136064
	2295603136832 -> 2295572754128
}
