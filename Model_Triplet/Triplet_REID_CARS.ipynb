{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Checking GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Histogram Image Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average dimensions in training images: (243.56360844936205, 214.27788660066705)\n",
      "Average dimensions in test images: (243.71741946627515, 217.27705328612143)\n"
     ]
    }
   ],
   "source": [
    "def process_image(file_path):\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            width, height = img.size\n",
    "            return width, height\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def collect_dimensions(image_dir):\n",
    "    dimensions = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for file_name in os.listdir(image_dir):\n",
    "            file_path = os.path.join(image_dir, file_name)\n",
    "            if os.path.isfile(file_path) and file_name.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "                futures.append(executor.submit(process_image, file_path))\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                dimensions.append(result)\n",
    "\n",
    "    return dimensions\n",
    "\n",
    "def plot_histogram(dimensions, title):\n",
    "    heights = [dim[1] for dim in dimensions]\n",
    "    widths = [dim[0] for dim in dimensions]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(heights, widths, alpha=0.7, label=\"Image Dimensions\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Height (pixels)\")\n",
    "    plt.ylabel(\"Width (pixels)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train_dir = \"D:\\\\Computer Vision\\\\FYP\\\\TASK 1\\\\env\\\\TrackNet-X\\\\DataSet\\\\VeRi\\\\image_train\"\n",
    "test_dir = \"D:\\\\Computer Vision\\\\FYP\\\\TASK 1\\\\env\\\\TrackNet-X\\\\DataSet\\\\VeRi\\\\image_test\"\n",
    "\n",
    "train_dimensions = collect_dimensions(train_dir)\n",
    "test_dimensions = collect_dimensions(test_dir)\n",
    "\n",
    "if train_dimensions:\n",
    "    plot_histogram(train_dimensions, \"Training Images: Height vs Width\")\n",
    "else:\n",
    "    print(\"No dimensions collected for training images.\")\n",
    "\n",
    "if test_dimensions:\n",
    "    plot_histogram(test_dimensions, \"Test Images: Height vs Width\")\n",
    "else:\n",
    "    print(\"No dimensions collected for test images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Loading & Triplet Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VERIDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for img_name in os.listdir(data_dir):\n",
    "            if img_name.endswith('.jpg'):\n",
    "                self.image_paths.append(img_name)\n",
    "                car_id = int(img_name.split('_')[0])\n",
    "                self.labels.append(car_id)\n",
    "\n",
    "        # Create a mapping for car IDs to indices\n",
    "        self.id_to_indices = {}\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            if label not in self.id_to_indices:\n",
    "                self.id_to_indices[label] = []\n",
    "            self.id_to_indices[label].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.data_dir, self.image_paths[index])\n",
    "        label = self.labels[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def get_triplet(self):\n",
    "        ids_list = random.sample(self.id_to_indices.keys(), 1)[0]\n",
    "        anchor_idx = random.choice(self.id_to_indices[ids_list])\n",
    "        positive_idx = random.choice(self.id_to_indices[ids_list])\n",
    "\n",
    "        # Ensure negative is from a different ID\n",
    "        negative_ids = list(self.id_to_indices.keys())\n",
    "        negative_ids.remove(ids_list)\n",
    "        negative_id = random.choice(negative_ids)\n",
    "        negative_idx = random.choice(self.id_to_indices[negative_id])\n",
    "\n",
    "        return anchor_idx, positive_idx, negative_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, veri_dataset):\n",
    "        self.veri_dataset = veri_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.veri_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_idx, positive_idx, negative_idx = self.veri_dataset.get_triplet()\n",
    "        anchor, _ = self.veri_dataset[anchor_idx]\n",
    "        positive, _ = self.veri_dataset[positive_idx]\n",
    "        negative, _ = self.veri_dataset[negative_idx]\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custom CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TripletCNN, self).__init__()\n",
    "\n",
    "        # First convolution layer: 7x7 kernel, stride 5, padding 3, 12 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=7, stride=5, padding=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)  # No downsampling here\n",
    "\n",
    "        # Second convolution layer: 3x3 kernel, stride 1, padding 1, 24 channels\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Third convolution layer: 3x3 kernel, stride 1, padding 1, 32 channels (no downsampling)\n",
    "        self.conv3 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces to 32x32\n",
    "\n",
    "        # Fourth convolution layer: 3x3 kernel, stride 1, padding 1, 64 channels (no downsampling)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Adaptive pooling to prevent size issues\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = self.pool1(x1)\n",
    "\n",
    "        x3 = F.relu(self.conv2(x2))\n",
    "        x4 = F.relu(self.conv3(x3))\n",
    "        x5 = self.pool2(x4)\n",
    "\n",
    "        x6 = F.relu(self.conv4(x5))\n",
    "        x7 = self.global_pool(x6)\n",
    "        x8 = self.fc(x7)\n",
    "\n",
    "        return x8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Triplet Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = torch.nn.functional.pairwise_distance(anchor, positive)\n",
    "        neg_dist = torch.nn.functional.pairwise_distance(anchor, negative)\n",
    "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "#     model.to(device)\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         for anchor, positive, negative in train_loader:\n",
    "#             anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             anchor_out = model(anchor)\n",
    "#             positive_out = model(positive)\n",
    "#             negative_out = model(negative)\n",
    "#             loss = criterion(anchor_out, positive_out, negative_out)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(anchor_out, positive_out, negative_out):\n",
    "    pos_dist = torch.nn.functional.pairwise_distance(anchor_out, positive_out)\n",
    "    neg_dist = torch.nn.functional.pairwise_distance(anchor_out, negative_out)\n",
    "    correct = (pos_dist < neg_dist).sum().item()\n",
    "    accuracy = correct / anchor_out.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device, epochs=10):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "        for anchor, positive, negative in train_loader:\n",
    "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            anchor_out = model(anchor)\n",
    "            positive_out = model(positive)\n",
    "            negative_out = model(negative)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            loss = criterion(anchor_out, positive_out, negative_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy = calculate_accuracy(anchor_out, positive_out, negative_out)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += accuracy\n",
    "\n",
    "        # Calculate average loss and accuracy for training\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "    return train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Accuracy and Loss Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, train_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', color='blue', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='green', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_16976\\2779224039.py:33: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  ids_list = random.sample(self.id_to_indices.keys(), 1)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.5859, Train Accuracy: 0.7473\n",
      "Epoch 2/10 - Train Loss: 0.4586, Train Accuracy: 0.8041\n",
      "Epoch 3/10 - Train Loss: 0.4261, Train Accuracy: 0.8198\n",
      "Epoch 4/10 - Train Loss: 0.4051, Train Accuracy: 0.8285\n",
      "Epoch 5/10 - Train Loss: 0.3863, Train Accuracy: 0.8370\n",
      "Epoch 6/10 - Train Loss: 0.3717, Train Accuracy: 0.8436\n",
      "Epoch 7/10 - Train Loss: 0.3586, Train Accuracy: 0.8485\n",
      "Epoch 8/10 - Train Loss: 0.3439, Train Accuracy: 0.8547\n",
      "Epoch 9/10 - Train Loss: 0.3292, Train Accuracy: 0.8598\n",
      "Epoch 10/10 - Train Loss: 0.3221, Train Accuracy: 0.8642\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_dir = r'D:\\Computer Vision\\FYP\\TASK 1\\env\\TrackNet-X\\DataSet\\VeRi\\image_train'\n",
    "    transform = transforms.Compose([transforms.Resize((196, 196)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    full_dataset = VERIDataset(data_dir, transform)\n",
    "\n",
    "    train_triplet_dataset = TripletDataset(full_dataset)\n",
    "    train_loader = DataLoader(train_triplet_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    model = TripletCNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = TripletLoss()\n",
    "\n",
    "    train_losses, train_accuracies = train_model(model, train_loader, optimizer, criterion, device, epochs=10)\n",
    "\n",
    "    plot_learning_curves(train_losses, train_accuracies)\n",
    "\n",
    "    torch.save(model.state_dict(), 'triplet_cnn.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Before & After Trining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplet_embeddings(model, dataset, device, before_training=True, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot embeddings before or after training for a triplet network using the anchor images.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The triplet model to generate embeddings.\n",
    "        dataset (TripletDataset): The dataset containing triplets.\n",
    "        device (torch.device): The device (CPU or GPU) for computation.\n",
    "        before_training (bool): If True, generates embeddings using an untrained model.\n",
    "        title_suffix (str): Additional string to append to the plot title.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    if not before_training:\n",
    "        model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, _, label in dataset:  # Only use anchor and its label\n",
    "            anchor = anchor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            embedding = model(anchor).cpu().numpy()  # Get embedding\n",
    "            embeddings.append(embedding)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Use t-SNE for dimensionality reduction to 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot embeddings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)\n",
    "        plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1], label=f\"Class {label}\")\n",
    "    \n",
    "    state = \"Before Training\" if before_training else \"After Training\"\n",
    "    plt.title(f\"{state} Embeddings {title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prediction and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are different (distance: 1.5968)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_16976\\1065226769.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path, transform):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Add batch dimension (1, C, H, W)\n",
    "    return img\n",
    "\n",
    "def compute_distance(model, image1_path, image2_path, transform, margin=1.0, device='cpu'):\n",
    "    image1 = preprocess_image(image1_path, transform).to(device)\n",
    "    image2 = preprocess_image(image2_path, transform).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature1 = model(image1)\n",
    "        feature2 = model(image2)\n",
    "    \n",
    "    distance = F.pairwise_distance(feature1, feature2).item()\n",
    "    similarity = distance < margin\n",
    "    \n",
    "    if similarity:\n",
    "        print(f\"Images are similar (distance: {distance:.4f})\")\n",
    "    else:\n",
    "        print(f\"Images are different (distance: {distance:.4f})\")\n",
    "    \n",
    "    return distance, similarity\n",
    "\n",
    "def main():\n",
    "\n",
    "    image1_path = r'D:\\Computer Vision\\FYP\\TASK 1\\env\\TrackNet-X\\DataSet\\VeRi\\image_test\\0135_c012_00060325_0.jpg'  \n",
    "    image2_path = r'D:\\Computer Vision\\FYP\\TASK 1\\env\\TrackNet-X\\DataSet\\VeRi\\image_test\\0135_c011_00069430_0.jpg'\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize((196, 196)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    model = TripletCNN().to(device)\n",
    "    model = load_model(model, model_path)\n",
    "    \n",
    "    distance, similarity = compute_distance(model, image1_path, image2_path, transform, margin=1.0, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_16976\\1065226769.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_16976\\2779224039.py:33: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  ids_list = random.sample(self.id_to_indices.keys(), 1)[0]\n"
     ]
    }
   ],
   "source": [
    "data_dir = r'D:\\Computer Vision\\FYP\\TASK 1\\env\\TrackNet-X\\DataSet\\VeRi\\image_train'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((196, 196)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Prepare dataset\n",
    "full_dataset = VERIDataset(data_dir, transform)\n",
    "dataset = TripletDataset(full_dataset)\n",
    "\n",
    "# Load model\n",
    "model_path = 'triplet_cnn.pth' \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TripletCNN().to(device)\n",
    "model = load_model(model, model_path)\n",
    "\n",
    "# Plot embeddings after training\n",
    "plot_triplet_embeddings(model, dataset, device, before_training=False, title_suffix=\"(Trained Model)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
