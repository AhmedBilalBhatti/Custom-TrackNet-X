{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VERIDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for img_name in os.listdir(data_dir):\n",
    "            if img_name.endswith('.jpg'):\n",
    "                self.image_paths.append(img_name)\n",
    "                car_id = int(img_name.split('_')[0])\n",
    "                self.labels.append(car_id)\n",
    "\n",
    "        # Create a mapping for car IDs to indices\n",
    "        self.id_to_indices = {}\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            if label not in self.id_to_indices:\n",
    "                self.id_to_indices[label] = []\n",
    "            self.id_to_indices[label].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.data_dir, self.image_paths[index])\n",
    "        label = self.labels[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def get_triplet(self):\n",
    "        ids_list = random.sample(self.id_to_indices.keys(), 1)[0]\n",
    "        anchor_idx = random.choice(self.id_to_indices[ids_list])\n",
    "        positive_idx = random.choice(self.id_to_indices[ids_list])\n",
    "\n",
    "        # Ensure negative is from a different ID\n",
    "        negative_ids = list(self.id_to_indices.keys())\n",
    "        negative_ids.remove(ids_list)\n",
    "        negative_id = random.choice(negative_ids)\n",
    "        negative_idx = random.choice(self.id_to_indices[negative_id])\n",
    "\n",
    "        return anchor_idx, positive_idx, negative_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, veri_dataset):\n",
    "        self.veri_dataset = veri_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.veri_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_idx, positive_idx, negative_idx = self.veri_dataset.get_triplet()\n",
    "        anchor, _ = self.veri_dataset[anchor_idx]\n",
    "        positive, _ = self.veri_dataset[positive_idx]\n",
    "        negative, _ = self.veri_dataset[negative_idx]\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TripletCNN, self).__init__()\n",
    "\n",
    "        # First convolution layer: 7x7 kernel, stride 5, padding 3, 12 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=7, stride=5, padding=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)  # No downsampling here\n",
    "\n",
    "        # Second convolution layer: 3x3 kernel, stride 1, padding 1, 24 channels\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Third convolution layer: 3x3 kernel, stride 1, padding 1, 32 channels (no downsampling)\n",
    "        self.conv3 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces to 32x32\n",
    "\n",
    "        # Fourth convolution layer: 3x3 kernel, stride 1, padding 1, 64 channels (no downsampling)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Adaptive pooling to prevent size issues\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = self.pool1(x1)\n",
    "\n",
    "        x3 = F.relu(self.conv2(x2))\n",
    "        x4 = F.relu(self.conv3(x3))\n",
    "        x5 = self.pool2(x4)\n",
    "\n",
    "        x6 = F.relu(self.conv4(x5))\n",
    "        x7 = self.global_pool(x6)\n",
    "        x8 = self.fc(x7)\n",
    "\n",
    "        return x8\n",
    "\n",
    "\n",
    "\n",
    "# Define Triplet Loss\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = torch.nn.functional.pairwise_distance(anchor, positive)\n",
    "        neg_dist = torch.nn.functional.pairwise_distance(anchor, negative)\n",
    "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for anchor, positive, negative in train_loader:\n",
    "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            anchor_out = model(anchor)\n",
    "            positive_out = model(positive)\n",
    "            negative_out = model(negative)\n",
    "            loss = criterion(anchor_out, positive_out, negative_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_19468\\1456088092.py:33: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  ids_list = random.sample(self.id_to_indices.keys(), 1)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3646\n",
      "Epoch 2/10, Train Loss: 0.2342\n",
      "Epoch 3/10, Train Loss: 0.2022\n",
      "Epoch 4/10, Train Loss: 0.1829\n",
      "Epoch 5/10, Train Loss: 0.1649\n",
      "Epoch 6/10, Train Loss: 0.1570\n",
      "Epoch 7/10, Train Loss: 0.1484\n",
      "Epoch 8/10, Train Loss: 0.1393\n",
      "Epoch 9/10, Train Loss: 0.1376\n",
      "Epoch 10/10, Train Loss: 0.1250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    data_dir = r'D:\\Computer Vision\\FYP\\TASK 1\\env\\TrackNet-X\\DataSet\\VeRi\\image_train'  # Change to your dataset path\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load the complete dataset\n",
    "    full_dataset = VERIDataset(data_dir, transform)\n",
    "\n",
    "    # Split dataset into train and validation sets\n",
    "    train_size = int(0.75 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_indices, val_indices = torch.utils.data.random_split(range(len(full_dataset)), [train_size, val_size])\n",
    "\n",
    "    # Create train and validation subsets\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # Wrap Subset objects with TripletDataset\n",
    "    train_triplet_dataset = TripletDataset(full_dataset)\n",
    "    val_triplet_dataset = TripletDataset(full_dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_triplet_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_triplet_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    model = TripletCNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = TripletLoss()\n",
    "\n",
    "    # Set device and start training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
