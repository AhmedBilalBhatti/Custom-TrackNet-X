{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(384, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"yolov8m.pt\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r'C:\\Users\\ahmad\\Downloads\\3623819-hd_1920_1080_25fp.mp4'\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_people = {}  # Dictionary to hold people (ID: (features, last known location))\n",
    "person_id = 1  # Start ID from 1\n",
    "frame_count = 0\n",
    "max_distance = 50  # Max distance for spatial matching\n",
    "max_feature_similarity = 0.8  # Minimum similarity threshold for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"Extract dominant color as a simple feature vector.\"\"\"\n",
    "    # Resize to 50x50 and calculate the mean color as a proxy for clothing color\n",
    "    resized = cv2.resize(image, (50, 50))\n",
    "    mean_color = resized.mean(axis=(0, 1))\n",
    "    return mean_color / 255  # Normalize color values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 2295.5ms\n",
      "Speed: 128.0ms preprocess, 2295.5ms inference, 36.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 88.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 1 suitcase, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 995.2ms\n",
      "Speed: 3.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 993.2ms\n",
      "Speed: 2.0ms preprocess, 993.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 996.2ms\n",
      "Speed: 3.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 4 handbags, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 cars, 5 handbags, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 6 handbags, 1007.2ms\n",
      "Speed: 2.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 990.2ms\n",
      "Speed: 2.0ms preprocess, 990.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 handbags, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 1 backpack, 4 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 996.2ms\n",
      "Speed: 3.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 1 frisbee, 1010.2ms\n",
      "Speed: 3.0ms preprocess, 1010.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 2 handbags, 999.2ms\n",
      "Speed: 4.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1008.2ms\n",
      "Speed: 2.0ms preprocess, 1008.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1016.2ms\n",
      "Speed: 2.0ms preprocess, 1016.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1006.2ms\n",
      "Speed: 3.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 6 handbags, 995.2ms\n",
      "Speed: 2.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 6 handbags, 998.2ms\n",
      "Speed: 3.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 1054.2ms\n",
      "Speed: 3.0ms preprocess, 1054.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 1 umbrella, 7 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 995.2ms\n",
      "Speed: 2.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 995.2ms\n",
      "Speed: 2.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 993.2ms\n",
      "Speed: 2.0ms preprocess, 993.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 5 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1 suitcase, 1013.2ms\n",
      "Speed: 2.0ms preprocess, 1013.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 992.2ms\n",
      "Speed: 2.0ms preprocess, 992.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1004.2ms\n",
      "Speed: 2.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 5 handbags, 1 suitcase, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1022.2ms\n",
      "Speed: 3.0ms preprocess, 1022.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1 chair, 1 clock, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1007.2ms\n",
      "Speed: 2.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 994.2ms\n",
      "Speed: 2.0ms preprocess, 994.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 2 handbags, 1006.2ms\n",
      "Speed: 3.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1 chair, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 backpacks, 2 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 backpacks, 2 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1 chair, 1029.2ms\n",
      "Speed: 2.0ms preprocess, 1029.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1002.2ms\n",
      "Speed: 4.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 994.2ms\n",
      "Speed: 2.0ms preprocess, 994.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 1028.2ms\n",
      "Speed: 2.0ms preprocess, 1028.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 3 handbags, 1004.2ms\n",
      "Speed: 3.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 5 handbags, 1001.2ms\n",
      "Speed: 4.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 4 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1 suitcase, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 1008.2ms\n",
      "Speed: 3.0ms preprocess, 1008.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 5 handbags, 1003.2ms\n",
      "Speed: 3.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 995.2ms\n",
      "Speed: 3.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 5 handbags, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 5 handbags, 1003.2ms\n",
      "Speed: 3.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 3 handbags, 1010.2ms\n",
      "Speed: 2.0ms preprocess, 1010.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 1005.2ms\n",
      "Speed: 3.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 1 umbrella, 6 handbags, 1 tennis racket, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Detect people using YOLOv8\n",
    "    results = model(frame)\n",
    "    boxes = results[0].boxes\n",
    "    person_boxes = boxes[boxes.cls == 0]  # Filter for people class (class ID 0)\n",
    "\n",
    "    current_detections = []\n",
    "    for box in person_boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        person_crop = frame[y1:y2, x1:x2]  # Crop person region for feature extraction\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        confidence = box.conf[0]\n",
    "        \n",
    "        # Extract features for each detected person\n",
    "        features = extract_features(person_crop)\n",
    "        current_detections.append((center_x, center_y, x1, y1, x2, y2, confidence, features))\n",
    "\n",
    "    # Match current detections with tracked people using features\n",
    "    new_tracked_people = {}\n",
    "    for center_x, center_y, x1, y1, x2, y2, confidence, features in current_detections:\n",
    "        matched = False\n",
    "\n",
    "        for id, data in tracked_people.items():\n",
    "            prev_center_x, prev_center_y, prev_features, last_frame = data\n",
    "\n",
    "            # Check spatial distance and feature similarity\n",
    "            distance = np.sqrt((center_x - prev_center_x) ** 2 + (center_y - prev_center_y) ** 2)\n",
    "            similarity = cosine_similarity([features], [prev_features])[0][0]\n",
    "            \n",
    "            if distance < max_distance and similarity > max_feature_similarity:\n",
    "                # Update tracked person with current detection\n",
    "                new_tracked_people[id] = (center_x, center_y, features, frame_count)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            # Assign a new ID for untracked person\n",
    "            new_tracked_people[person_id] = (center_x, center_y, features, frame_count)\n",
    "            person_id += 1\n",
    "\n",
    "    # Update tracked people with new detections\n",
    "    tracked_people = new_tracked_people\n",
    "\n",
    "    # Draw tracking results on the frame\n",
    "    for id, (center_x, center_y, features, last_frame) in tracked_people.items():\n",
    "        # Retrieve bounding box coordinates from current_detections for each ID\n",
    "        # and display with unique ID and confidence score.\n",
    "        for detection in current_detections:\n",
    "            det_center_x, det_center_y, det_x1, det_y1, det_x2, det_y2, det_confidence, det_features = detection\n",
    "            if center_x == det_center_x and center_y == det_center_y:\n",
    "                # Draw bounding box and label with ID\n",
    "                cv2.rectangle(frame, (det_x1, det_y1), (det_x2, det_y2), (0, 255, 0), 2)\n",
    "                label = f'ID: {id} Conf: {det_confidence:.2f}'\n",
    "                cv2.putText(frame, label, (det_x1, det_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                break\n",
    "\n",
    "    cv2.imshow('Tracked People', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Computer Vision\\FYP\\TASK 1\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Computer Vision\\FYP\\TASK 1\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "d:\\Computer Vision\\FYP\\TASK 1\\env\\lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 382.1ms\n",
      "Speed: 3.0ms preprocess, 382.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 353.1ms\n",
      "Speed: 2.0ms preprocess, 353.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 1 suitcase, 314.1ms\n",
      "Speed: 3.0ms preprocess, 314.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 330.1ms\n",
      "Speed: 2.0ms preprocess, 330.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 301.1ms\n",
      "Speed: 1.0ms preprocess, 301.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 365.1ms\n",
      "Speed: 2.0ms preprocess, 365.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 4 handbags, 371.1ms\n",
      "Speed: 3.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 cars, 5 handbags, 366.1ms\n",
      "Speed: 1.0ms preprocess, 366.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 6 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 366.1ms\n",
      "Speed: 3.0ms preprocess, 366.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 364.1ms\n",
      "Speed: 2.0ms preprocess, 364.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 370.1ms\n",
      "Speed: 1.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 369.1ms\n",
      "Speed: 1.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 370.1ms\n",
      "Speed: 1.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 371.1ms\n",
      "Speed: 3.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 1 backpack, 4 handbags, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 366.1ms\n",
      "Speed: 2.0ms preprocess, 366.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 1 frisbee, 352.1ms\n",
      "Speed: 3.0ms preprocess, 352.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 2 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 369.1ms\n",
      "Speed: 3.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 6 handbags, 366.1ms\n",
      "Speed: 3.0ms preprocess, 366.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 6 handbags, 362.1ms\n",
      "Speed: 2.0ms preprocess, 362.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 365.1ms\n",
      "Speed: 3.0ms preprocess, 365.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 370.1ms\n",
      "Speed: 1.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 373.1ms\n",
      "Speed: 3.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 374.1ms\n",
      "Speed: 3.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 370.1ms\n",
      "Speed: 3.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 1 umbrella, 7 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 365.1ms\n",
      "Speed: 2.0ms preprocess, 365.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 368.1ms\n",
      "Speed: 2.0ms preprocess, 368.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 364.1ms\n",
      "Speed: 2.0ms preprocess, 364.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 375.1ms\n",
      "Speed: 2.0ms preprocess, 375.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 5 handbags, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 368.1ms\n",
      "Speed: 2.0ms preprocess, 368.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1 suitcase, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 366.1ms\n",
      "Speed: 2.0ms preprocess, 366.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 5 handbags, 1 suitcase, 376.1ms\n",
      "Speed: 1.0ms preprocess, 376.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 372.1ms\n",
      "Speed: 3.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 3 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1 chair, 1 clock, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 375.1ms\n",
      "Speed: 2.0ms preprocess, 375.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 363.1ms\n",
      "Speed: 2.0ms preprocess, 363.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 2 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1 chair, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 backpacks, 2 handbags, 373.1ms\n",
      "Speed: 1.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 backpacks, 2 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1 chair, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 371.1ms\n",
      "Speed: 3.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 372.1ms\n",
      "Speed: 3.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 362.1ms\n",
      "Speed: 2.0ms preprocess, 362.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 3 handbags, 364.1ms\n",
      "Speed: 3.0ms preprocess, 364.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 368.1ms\n",
      "Speed: 2.0ms preprocess, 368.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 5 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 4 handbags, 394.1ms\n",
      "Speed: 3.0ms preprocess, 394.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1 suitcase, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 365.1ms\n",
      "Speed: 3.0ms preprocess, 365.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 365.1ms\n",
      "Speed: 3.0ms preprocess, 365.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 5 handbags, 365.1ms\n",
      "Speed: 1.0ms preprocess, 365.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 370.1ms\n",
      "Speed: 1.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 5 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 388.1ms\n",
      "Speed: 2.0ms preprocess, 388.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 5 handbags, 393.1ms\n",
      "Speed: 2.0ms preprocess, 393.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 366.1ms\n",
      "Speed: 3.0ms preprocess, 366.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 3 handbags, 363.1ms\n",
      "Speed: 2.0ms preprocess, 363.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 1 umbrella, 6 handbags, 1 tennis racket, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 370.1ms\n",
      "Speed: 1.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 376.1ms\n",
      "Speed: 3.0ms preprocess, 376.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 6 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 375.1ms\n",
      "Speed: 2.0ms preprocess, 375.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 6 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 6 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 370.1ms\n",
      "Speed: 3.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 2 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 handbags, 1 frisbee, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 367.1ms\n",
      "Speed: 2.0ms preprocess, 367.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 1 backpack, 4 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 4 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 5 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 374.1ms\n",
      "Speed: 2.0ms preprocess, 374.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 4 handbags, 376.1ms\n",
      "Speed: 1.0ms preprocess, 376.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 375.1ms\n",
      "Speed: 2.0ms preprocess, 375.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 4 handbags, 371.1ms\n",
      "Speed: 1.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 handbags, 372.1ms\n",
      "Speed: 2.0ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 6 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 cars, 5 handbags, 1 baseball glove, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 370.1ms\n",
      "Speed: 2.0ms preprocess, 370.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 369.1ms\n",
      "Speed: 1.0ms preprocess, 369.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 371.1ms\n",
      "Speed: 2.0ms preprocess, 371.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 375.1ms\n",
      "Speed: 2.0ms preprocess, 375.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 375.1ms\n",
      "Speed: 2.0ms preprocess, 375.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 369.1ms\n",
      "Speed: 2.0ms preprocess, 369.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 368.1ms\n",
      "Speed: 2.0ms preprocess, 368.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 2 handbags, 373.1ms\n",
      "Speed: 2.0ms preprocess, 373.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 3 handbags, 1 suitcase, 368.1ms\n",
      "Speed: 2.0ms preprocess, 368.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ultralytics import YOLO\n",
    "from torchvision import models\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8m.pt\")\n",
    "yolo_model.to('cpu')\n",
    "\n",
    "# Define the path to the locally stored checkpoint model\n",
    "checkpoint_path = r'C:\\Users\\ahmad\\.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth'\n",
    "\n",
    "# Load the InceptionV3 model from torchvision\n",
    "model = models.inception_v3(pretrained=False)  # Do not load the weights automatically\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load the checkpoint into the model\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize video capture\n",
    "video_path = r'C:\\Users\\ahmad\\Downloads\\3623819-hd_1920_1080_25fp.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize variables\n",
    "tracked_people = {}  # Dictionary to hold people (ID: (features, last known location, frame_count))\n",
    "person_id = 1  # Start ID from 1\n",
    "frame_count = 0\n",
    "max_distance = 50  # Max distance for spatial matching\n",
    "max_feature_similarity = 0.8  # Minimum similarity threshold for matching\n",
    "\n",
    "def extract_features(image):\n",
    "    \"\"\"Extract unique features using the custom checkpoint model.\"\"\"\n",
    "    # Preprocess image (resize and normalize)\n",
    "    image_resized = cv2.resize(image, (299, 299))  # Resize to match InceptionV3 input size\n",
    "    image_resized = np.transpose(image_resized, (2, 0, 1))  # Convert to CHW format\n",
    "    image_resized = torch.tensor(image_resized).float() / 255.0  # Normalize to [0, 1]\n",
    "    image_resized = image_resized.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        features = model(image_resized)  # Get the features from the model\n",
    "    return features.cpu().numpy().flatten()  # Flatten the feature vector for comparison\n",
    "\n",
    "def get_person_id(features):\n",
    "    \"\"\"Check if the current features match with any previous person ID.\"\"\"\n",
    "    max_similarity = 0\n",
    "    matched_id = None\n",
    "    for id, (saved_features, _) in tracked_people.items():\n",
    "        similarity = cosine_similarity([features], [saved_features])[0][0]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            matched_id = id\n",
    "    return matched_id, max_similarity\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Detect people using YOLOv8\n",
    "    results = yolo_model(frame)\n",
    "    boxes = results[0].boxes\n",
    "    person_boxes = boxes[boxes.cls == 0]  # Filter for people class (class ID 0)\n",
    "\n",
    "    current_detections = []\n",
    "    for box in person_boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        person_crop = frame[y1:y2, x1:x2]  # Crop person region for feature extraction\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        confidence = box.conf[0]\n",
    "\n",
    "        # Extract features for each detected person using the custom model\n",
    "        features = extract_features(person_crop)\n",
    "        current_detections.append((center_x, center_y, x1, y1, x2, y2, confidence, features))\n",
    "\n",
    "    new_tracked_people = {}\n",
    "    for center_x, center_y, x1, y1, x2, y2, confidence, features in current_detections:\n",
    "        matched_id, similarity = get_person_id(features)\n",
    "\n",
    "        if matched_id is not None and similarity > max_feature_similarity:\n",
    "            # Reassign the ID if the person matches an existing ID\n",
    "            new_tracked_people[matched_id] = (features, frame_count)\n",
    "        else:\n",
    "            # Assign a new ID if no match is found\n",
    "            new_tracked_people[person_id] = (features, frame_count)\n",
    "            person_id += 1\n",
    "\n",
    "    # Update tracked people with new detections\n",
    "    tracked_people = new_tracked_people\n",
    "\n",
    "    # Draw tracking results on the frame\n",
    "    for id, (features, _) in tracked_people.items():\n",
    "        for detection in current_detections:\n",
    "            det_center_x, det_center_y, det_x1, det_y1, det_x2, det_y2, det_confidence, det_features = detection\n",
    "            similarity = cosine_similarity([features], [det_features])[0][0]  # Compare features with cosine similarity\n",
    "            if similarity > max_feature_similarity:  # If similarity is good enough, mark the person\n",
    "                # Draw bounding box and label with ID\n",
    "                cv2.rectangle(frame, (det_x1, det_y1), (det_x2, det_y2), (0, 255, 0), 2)\n",
    "                label = f'ID: {id} Conf: {det_confidence:.2f}'\n",
    "                cv2.putText(frame, label, (det_x1, det_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                break\n",
    "\n",
    "    # Display the resulting frame with tracking\n",
    "    cv2.imshow('Tracked People', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
