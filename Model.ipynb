{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8m.pt\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r'C:\\Users\\ahmad\\Downloads\\3623819-hd_1920_1080_25fps.mp4'\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_people = {}  # Dictionary to hold people (ID: (features, last known location))\n",
    "person_id = 1  # Start ID from 1\n",
    "frame_count = 0\n",
    "max_distance = 50  # Max distance for spatial matching\n",
    "max_feature_similarity = 0.8  # Minimum similarity threshold for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"Extract dominant color as a simple feature vector.\"\"\"\n",
    "    # Resize to 50x50 and calculate the mean color as a proxy for clothing color\n",
    "    resized = cv2.resize(image, (50, 50))\n",
    "    mean_color = resized.mean(axis=(0, 1))\n",
    "    return mean_color / 255  # Normalize color values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 1031.8ms\n",
      "Speed: 4.0ms preprocess, 1031.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1012.3ms\n",
      "Speed: 4.0ms preprocess, 1012.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 4 handbags, 1 suitcase, 1023.3ms\n",
      "Speed: 3.0ms preprocess, 1023.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1009.8ms\n",
      "Speed: 2.0ms preprocess, 1009.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1009.3ms\n",
      "Speed: 2.0ms preprocess, 1009.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 1011.3ms\n",
      "Speed: 2.0ms preprocess, 1011.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 1001.3ms\n",
      "Speed: 2.0ms preprocess, 1001.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 4 handbags, 1 skateboard, 997.3ms\n",
      "Speed: 2.0ms preprocess, 997.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 3 handbags, 1003.3ms\n",
      "Speed: 2.0ms preprocess, 1003.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 backpack, 4 handbags, 1004.3ms\n",
      "Speed: 3.0ms preprocess, 1004.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 1001.3ms\n",
      "Speed: 2.0ms preprocess, 1001.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 handbags, 1 baseball glove, 996.3ms\n",
      "Speed: 3.0ms preprocess, 996.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 998.3ms\n",
      "Speed: 2.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 2 handbags, 1005.3ms\n",
      "Speed: 2.0ms preprocess, 1005.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 5 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 handbags, 1004.3ms\n",
      "Speed: 2.0ms preprocess, 1004.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 998.3ms\n",
      "Speed: 3.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 5 handbags, 998.3ms\n",
      "Speed: 2.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 998.3ms\n",
      "Speed: 2.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 handbags, 1007.3ms\n",
      "Speed: 2.0ms preprocess, 1007.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 handbags, 1004.3ms\n",
      "Speed: 2.0ms preprocess, 1004.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 996.3ms\n",
      "Speed: 2.0ms preprocess, 996.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 5 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 5 handbags, 1 frisbee, 1001.3ms\n",
      "Speed: 2.0ms preprocess, 1001.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1000.3ms\n",
      "Speed: 3.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1009.3ms\n",
      "Speed: 2.0ms preprocess, 1009.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 996.3ms\n",
      "Speed: 3.0ms preprocess, 996.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 handbags, 995.3ms\n",
      "Speed: 2.0ms preprocess, 995.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 995.3ms\n",
      "Speed: 2.0ms preprocess, 995.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 1002.3ms\n",
      "Speed: 4.0ms preprocess, 1002.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 7 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 backpacks, 5 handbags, 1043.8ms\n",
      "Speed: 2.0ms preprocess, 1043.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 5 handbags, 1002.3ms\n",
      "Speed: 3.0ms preprocess, 1002.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 996.3ms\n",
      "Speed: 3.0ms preprocess, 996.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 5 handbags, 1005.3ms\n",
      "Speed: 2.0ms preprocess, 1005.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 1 umbrella, 6 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 998.3ms\n",
      "Speed: 2.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1008.3ms\n",
      "Speed: 2.0ms preprocess, 1008.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 997.3ms\n",
      "Speed: 2.0ms preprocess, 997.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 3 handbags, 1003.3ms\n",
      "Speed: 2.0ms preprocess, 1003.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 1004.3ms\n",
      "Speed: 2.0ms preprocess, 1004.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 4 handbags, 1012.8ms\n",
      "Speed: 2.0ms preprocess, 1012.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 1002.3ms\n",
      "Speed: 2.0ms preprocess, 1002.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1 suitcase, 1006.3ms\n",
      "Speed: 2.0ms preprocess, 1006.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 5 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 4 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1001.3ms\n",
      "Speed: 2.0ms preprocess, 1001.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1010.3ms\n",
      "Speed: 2.0ms preprocess, 1010.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1022.8ms\n",
      "Speed: 3.0ms preprocess, 1022.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 997.3ms\n",
      "Speed: 2.0ms preprocess, 997.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 3 handbags, 1015.3ms\n",
      "Speed: 2.0ms preprocess, 1015.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 1 clock, 998.3ms\n",
      "Speed: 2.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 chair, 1002.3ms\n",
      "Speed: 2.0ms preprocess, 1002.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 997.3ms\n",
      "Speed: 2.0ms preprocess, 997.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1003.3ms\n",
      "Speed: 2.0ms preprocess, 1003.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 998.3ms\n",
      "Speed: 2.0ms preprocess, 998.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1000.3ms\n",
      "Speed: 2.0ms preprocess, 1000.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1010.3ms\n",
      "Speed: 2.0ms preprocess, 1010.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 999.3ms\n",
      "Speed: 2.0ms preprocess, 999.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 995.3ms\n",
      "Speed: 2.0ms preprocess, 995.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 backpacks, 2 handbags, 1001.3ms\n",
      "Speed: 2.0ms preprocess, 1001.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 1 chair, 997.3ms\n",
      "Speed: 3.0ms preprocess, 997.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 1019.8ms\n",
      "Speed: 3.0ms preprocess, 1019.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 1 backpack, 2 handbags, 1022.3ms\n",
      "Speed: 3.0ms preprocess, 1022.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 handbags, 1 chair, 1 cell phone, 1010.3ms\n",
      "Speed: 2.0ms preprocess, 1010.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Detect people using YOLOv8\n",
    "    results = model(frame)\n",
    "    boxes = results[0].boxes\n",
    "    person_boxes = boxes[boxes.cls == 0]  # Filter for people class (class ID 0)\n",
    "\n",
    "    current_detections = []\n",
    "    for box in person_boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        person_crop = frame[y1:y2, x1:x2]  # Crop person region for feature extraction\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        confidence = box.conf[0]\n",
    "        \n",
    "        # Extract features for each detected person\n",
    "        features = extract_features(person_crop)\n",
    "        current_detections.append((center_x, center_y, x1, y1, x2, y2, confidence, features))\n",
    "\n",
    "    # Match current detections with tracked people using features\n",
    "    new_tracked_people = {}\n",
    "    for center_x, center_y, x1, y1, x2, y2, confidence, features in current_detections:\n",
    "        matched = False\n",
    "\n",
    "        for id, data in tracked_people.items():\n",
    "            prev_center_x, prev_center_y, prev_features, last_frame = data\n",
    "\n",
    "            # Check spatial distance and feature similarity\n",
    "            distance = np.sqrt((center_x - prev_center_x) ** 2 + (center_y - prev_center_y) ** 2)\n",
    "            similarity = cosine_similarity([features], [prev_features])[0][0]\n",
    "            \n",
    "            if distance < max_distance and similarity > max_feature_similarity:\n",
    "                # Update tracked person with current detection\n",
    "                new_tracked_people[id] = (center_x, center_y, features, frame_count)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            # Assign a new ID for untracked person\n",
    "            new_tracked_people[person_id] = (center_x, center_y, features, frame_count)\n",
    "            person_id += 1\n",
    "\n",
    "    # Update tracked people with new detections\n",
    "    tracked_people = new_tracked_people\n",
    "\n",
    "    # Draw tracking results on the frame\n",
    "    for id, (center_x, center_y, features, last_frame) in tracked_people.items():\n",
    "        # Retrieve bounding box coordinates from current_detections for each ID\n",
    "        # and display with unique ID and confidence score.\n",
    "        for detection in current_detections:\n",
    "            det_center_x, det_center_y, det_x1, det_y1, det_x2, det_y2, det_confidence, det_features = detection\n",
    "            if center_x == det_center_x and center_y == det_center_y:\n",
    "                # Draw bounding box and label with ID\n",
    "                cv2.rectangle(frame, (det_x1, det_y1), (det_x2, det_y2), (0, 255, 0), 2)\n",
    "                label = f'ID: {id} Conf: {det_confidence:.2f}'\n",
    "                cv2.putText(frame, label, (det_x1, det_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                break\n",
    "\n",
    "    cv2.imshow('Tracked People', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
