{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.1  # Confidence threshold for detecting persons\n",
    "IOU_THRESHOLD = 0.1  # IoU threshold for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 1 handbag, 1027.2ms\n",
      "Speed: 4.0ms preprocess, 1027.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 1004.2ms\n",
      "Speed: 3.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 1 handbag, 1033.2ms\n",
      "Speed: 3.0ms preprocess, 1033.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1016.2ms\n",
      "Speed: 4.0ms preprocess, 1016.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 backpack, 1081.2ms\n",
      "Speed: 2.0ms preprocess, 1081.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 handbags, 1020.2ms\n",
      "Speed: 3.0ms preprocess, 1020.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 1 handbag, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 handbag, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 handbags, 1015.2ms\n",
      "Speed: 2.0ms preprocess, 1015.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1017.2ms\n",
      "Speed: 3.0ms preprocess, 1017.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1037.2ms\n",
      "Speed: 2.0ms preprocess, 1037.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 handbag, 1016.2ms\n",
      "Speed: 3.0ms preprocess, 1016.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 handbags, 1012.2ms\n",
      "Speed: 2.0ms preprocess, 1012.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 backpack, 3 handbags, 1019.2ms\n",
      "Speed: 2.0ms preprocess, 1019.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 3 handbags, 1014.2ms\n",
      "Speed: 3.0ms preprocess, 1014.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 handbags, 1023.2ms\n",
      "Speed: 3.0ms preprocess, 1023.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 1 handbag, 1013.2ms\n",
      "Speed: 3.0ms preprocess, 1013.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 handbag, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "    inter_x1 = max(x1, x1_2)\n",
    "    inter_y1 = max(y1, y1_2)\n",
    "    inter_x2 = min(x2, x2_2)\n",
    "    inter_y2 = min(y2, y2_2)\n",
    "    \n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    \n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    \n",
    "    # Compute the union area\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    # Compute IoU\n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "# Function to extract a simple feature from the person's bounding box (e.g., color histogram)\n",
    "def extract_features(frame, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    person_img = frame[y1:y2, x1:x2]  # Crop the person from the frame\n",
    "    # Convert to HSV for better color distribution matching\n",
    "    person_img_hsv = cv2.cvtColor(person_img, cv2.COLOR_BGR2HSV)\n",
    "    # Compute a normalized color histogram\n",
    "    hist = cv2.calcHist([person_img_hsv], [0, 1], None, [256, 256], [0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()  # Flatten to a 1D vector\n",
    "    return hist\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(r'D:\\Computer Vision\\FYP\\TASK 1\\env\\TrackNet-X\\yolov8m.pt')\n",
    "model.to('cpu')\n",
    "\n",
    "# Open video file\n",
    "cap = cv2.VideoCapture(r'C:\\Users\\ahmad\\Downloads\\853889-hd_1920_1080_25fps.mp4')\n",
    "\n",
    "# To store the unique IDs, bounding boxes, and their feature vectors\n",
    "person_ids = []\n",
    "person_features = []\n",
    "unique_id = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform inference on the frame\n",
    "    results = model([frame_rgb])  # Pass the frame as a batch of one image\n",
    "    result = results[0]  # Get the first result\n",
    "    boxes = result.boxes  # Get bounding boxes\n",
    "\n",
    "    xywh = boxes.xywh  # Get coordinates in xywh format\n",
    "    conf = boxes.conf  # Get the confidence scores\n",
    "    cls = boxes.cls  # Get the class labels\n",
    "\n",
    "    person_bboxes = []\n",
    "    for i in range(len(cls)):\n",
    "        if cls[i] == 0:  # Class 0 is 'person'\n",
    "            x, y, w, h = xywh[i]\n",
    "            confidence = conf[i]\n",
    "            x1, y1 = int(x - w / 2), int(y - h / 2)  # Convert to top-left corner\n",
    "            x2, y2 = int(x1 + w), int(y1 + h)  # Calculate bottom-right corner\n",
    "\n",
    "            person_bboxes.append((x1, y1, x2, y2, confidence))\n",
    "\n",
    "    updated_person_ids = []\n",
    "    updated_person_features = []\n",
    "    \n",
    "    for bbox in person_bboxes:\n",
    "        x1, y1, x2, y2, confidence = bbox\n",
    "        matched = False\n",
    "        current_feature = extract_features(frame, (x1, y1, x2, y2))  # Extract features of the current person\n",
    "        \n",
    "        # Match current person's feature with the stored ones\n",
    "# Match current person's feature with the stored ones\n",
    "        for pid, (prev_bbox, prev_feature) in enumerate(zip(person_ids, person_features)):\n",
    "            # Unpack bounding box and confidence\n",
    "            prev_x1, prev_y1, prev_x2, prev_y2, prev_confidence = prev_bbox\n",
    "            iou = compute_iou((x1, y1, x2, y2), (prev_x1, prev_y1, prev_x2, prev_y2))\n",
    "            \n",
    "            # If the IoU is high and features match, keep the same ID\n",
    "            if iou > 0.3:\n",
    "                feature_distance = np.linalg.norm(current_feature - prev_feature)  # Compute distance between features\n",
    "                if feature_distance < 0.5:  # If feature similarity is high enough\n",
    "                    updated_person_ids.append((bbox, pid, confidence))\n",
    "                    updated_person_features.append(prev_feature)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "        \n",
    "        # If no match was found, assign a new ID and feature\n",
    "        if not matched:\n",
    "            updated_person_ids.append((bbox, unique_id, confidence))\n",
    "            updated_person_features.append(current_feature)\n",
    "            unique_id += 1\n",
    "\n",
    "    # Ensure IDs are kept the same for each person across frames\n",
    "    person_ids = [bbox for bbox, _, _ in updated_person_ids]\n",
    "    person_features = updated_person_features\n",
    "\n",
    "    # Draw bounding boxes and person IDs on the frame\n",
    "# Draw bounding boxes and person IDs on the frame\n",
    "# Draw bounding boxes and person IDs on the frame\n",
    "    for (bbox, person_id, confidence) in updated_person_ids:\n",
    "        x1, y1, x2, y2 = bbox[:4]  # Unpack only the first four values (x1, y1, x2, y2)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'ID:{person_id} Conf:{confidence:.2f}', (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Show the frame with bounding boxes and IDs\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
