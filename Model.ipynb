{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 1 handbag, 1025.2ms\n",
      "Speed: 2.0ms preprocess, 1025.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 handbag, 1032.2ms\n",
      "Speed: 4.0ms preprocess, 1032.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 handbag, 1024.2ms\n",
      "Speed: 2.0ms preprocess, 1024.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1012.2ms\n",
      "Speed: 3.0ms preprocess, 1012.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 backpack, 1030.2ms\n",
      "Speed: 3.0ms preprocess, 1030.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1015.2ms\n",
      "Speed: 3.0ms preprocess, 1015.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1014.2ms\n",
      "Speed: 3.0ms preprocess, 1014.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 handbags, 1054.2ms\n",
      "Speed: 3.0ms preprocess, 1054.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 1 handbag, 1027.2ms\n",
      "Speed: 2.0ms preprocess, 1027.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 handbag, 1021.2ms\n",
      "Speed: 2.0ms preprocess, 1021.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 handbags, 1033.2ms\n",
      "Speed: 2.0ms preprocess, 1033.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 1019.2ms\n",
      "Speed: 2.0ms preprocess, 1019.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1011.2ms\n",
      "Speed: 3.0ms preprocess, 1011.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 handbag, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 backpack, 3 handbags, 1007.2ms\n",
      "Speed: 2.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 3 handbags, 1015.2ms\n",
      "Speed: 2.0ms preprocess, 1015.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 handbags, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 1 handbag, 1041.2ms\n",
      "Speed: 2.0ms preprocess, 1041.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 handbag, 1012.2ms\n",
      "Speed: 3.0ms preprocess, 1012.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 handbag, 1014.2ms\n",
      "Speed: 2.0ms preprocess, 1014.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 1 backpack, 1 handbag, 1007.2ms\n",
      "Speed: 2.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 handbag, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 3 handbags, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 handbags, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 3 handbags, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 31 persons, 1 backpack, 4 handbags, 1002.2ms\n",
      "Speed: 3.0ms preprocess, 1002.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 5 handbags, 1008.2ms\n",
      "Speed: 3.0ms preprocess, 1008.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 4 handbags, 1018.2ms\n",
      "Speed: 2.0ms preprocess, 1018.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 1 backpack, 4 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 backpack, 2 handbags, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 backpacks, 2 handbags, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 backpack, 2 handbags, 1004.2ms\n",
      "Speed: 2.0ms preprocess, 1004.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 bird, 1 backpack, 2 handbags, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 backpack, 1 handbag, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 1 backpack, 2 handbags, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 backpack, 3 handbags, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 4 handbags, 1000.2ms\n",
      "Speed: 3.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 3 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 5 handbags, 1003.2ms\n",
      "Speed: 3.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 3 handbags, 1004.2ms\n",
      "Speed: 2.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 3 handbags, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 4 handbags, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 4 handbags, 1 tennis racket, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 4 handbags, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 handbags, 1 tennis racket, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 2 handbags, 1 tennis racket, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 3 handbags, 1005.2ms\n",
      "Speed: 3.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 backpack, 1 handbag, 1023.2ms\n",
      "Speed: 3.0ms preprocess, 1023.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 handbags, 1005.2ms\n",
      "Speed: 3.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 handbags, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 1 backpack, 1 handbag, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 backpack, 4 handbags, 1019.2ms\n",
      "Speed: 4.0ms preprocess, 1019.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 backpack, 2 handbags, 1 suitcase, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 3 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 1 handbag, 996.2ms\n",
      "Speed: 3.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 2 backpacks, 1 handbag, 1 tennis racket, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 1 backpack, 3 handbags, 1 tennis racket, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 1 backpack, 2 handbags, 1 tennis racket, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 38 persons, 2 handbags, 995.2ms\n",
      "Speed: 3.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 38 persons, 2 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 2 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 backpacks, 1 handbag, 1004.2ms\n",
      "Speed: 3.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 2 backpacks, 3 handbags, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 32 persons, 3 handbags, 1004.2ms\n",
      "Speed: 2.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 3 handbags, 1001.2ms\n",
      "Speed: 3.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8m.pt\")  # Path to your YOLOv8 model\n",
    "model.to('cpu') \n",
    "\n",
    "# Initialize video capture\n",
    "video_path = r'C:\\Users\\ahmad\\Downloads\\853889-hd_1920_1080_25fps.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize variables\n",
    "tracked_people = []  # List to hold tracked people (ID, x, y, last_frame)\n",
    "person_id = 0  # Start with ID 0\n",
    "frame_count = 0\n",
    "max_distance = 50  # Max distance to consider matching people (can be tuned)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Detect people using YOLOv8\n",
    "    results = model(frame)  # Results of detections\n",
    "    boxes = results[0].boxes  # Get bounding boxes\n",
    "\n",
    "    # Filter for \"person\" class (class ID 0)\n",
    "    person_boxes = boxes[boxes.cls == 0]\n",
    "\n",
    "    current_detections = []\n",
    "    for box in person_boxes:\n",
    "        # Extract bounding box and calculate the center\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "\n",
    "        current_detections.append((center_x, center_y))\n",
    "\n",
    "    # Track people manually by comparing current detections with previously tracked people\n",
    "    new_tracked_people = []\n",
    "    for (center_x, center_y) in current_detections:\n",
    "        matched = False\n",
    "        for person in tracked_people:\n",
    "            prev_id, prev_x, prev_y, last_frame = person\n",
    "            distance = np.sqrt((center_x - prev_x) ** 2 + (center_y - prev_y) ** 2)\n",
    "            if distance < max_distance:\n",
    "                new_tracked_people.append((prev_id, center_x, center_y, frame_count))\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            # Assign a new ID for this person\n",
    "            new_tracked_people.append((person_id, center_x, center_y, frame_count))\n",
    "            person_id += 1\n",
    "\n",
    "    # Update the list of tracked people\n",
    "    tracked_people = new_tracked_people\n",
    "\n",
    "    # Draw the results on the frame\n",
    "    for person in tracked_people:\n",
    "        person_id, center_x, center_y, _ = person\n",
    "        cv2.circle(frame, (center_x, center_y), 5, (0, 255, 0), -1)\n",
    "        cv2.putText(frame, f'ID: {person_id}', (center_x, center_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the output frame\n",
    "    cv2.imshow('Tracked People', frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 25 persons, 1 airplane, 2 backpacks, 5 handbags, 1041.2ms\n",
      "Speed: 4.0ms preprocess, 1041.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 1 backpack, 5 handbags, 999.2ms\n",
      "Speed: 4.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 airplane, 1 backpack, 3 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 1 backpack, 5 handbags, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 1 backpack, 3 handbags, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 6 handbags, 1015.2ms\n",
      "Speed: 2.0ms preprocess, 1015.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 5 handbags, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 3 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 2 handbags, 1030.2ms\n",
      "Speed: 3.0ms preprocess, 1030.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 backpack, 4 handbags, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 backpack, 3 handbags, 1004.2ms\n",
      "Speed: 2.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 backpack, 5 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 4 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 backpack, 3 handbags, 1001.2ms\n",
      "Speed: 3.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 backpacks, 2 handbags, 995.2ms\n",
      "Speed: 2.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 3 backpacks, 4 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 backpacks, 5 handbags, 995.2ms\n",
      "Speed: 2.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 backpacks, 4 handbags, 993.2ms\n",
      "Speed: 3.0ms preprocess, 993.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 2 backpacks, 2 handbags, 996.2ms\n",
      "Speed: 2.0ms preprocess, 996.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 airplane, 1 backpack, 3 handbags, 995.2ms\n",
      "Speed: 3.0ms preprocess, 995.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 boat, 1 backpack, 1 handbag, 997.2ms\n",
      "Speed: 2.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 boat, 1 backpack, 1 handbag, 993.2ms\n",
      "Speed: 3.0ms preprocess, 993.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1 airplane, 1 boat, 2 backpacks, 2 handbags, 992.2ms\n",
      "Speed: 3.0ms preprocess, 992.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 boat, 2 backpacks, 1 handbag, 1001.2ms\n",
      "Speed: 3.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 1 boat, 2 backpacks, 2 handbags, 1001.2ms\n",
      "Speed: 3.0ms preprocess, 1001.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8m.pt\")  # Path to your YOLOv8 model\n",
    "model.to('cpu') \n",
    "\n",
    "# Initialize video capture\n",
    "video_path = r'C:\\Users\\ahmad\\Downloads\\1338598-hd_1920_1080_30fps.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize variables\n",
    "tracked_people = []  # List to hold tracked people (ID, x, y, last_frame)\n",
    "person_id = 0  # Start with ID 0\n",
    "frame_count = 0\n",
    "max_distance = 50  # Max distance to consider matching people (can be tuned)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Detect people using YOLOv8\n",
    "    results = model(frame)  # Results of detections\n",
    "    boxes = results[0].boxes  # Get bounding boxes\n",
    "\n",
    "    # Filter for \"person\" class (class ID 0)\n",
    "    person_boxes = boxes[boxes.cls == 0]\n",
    "\n",
    "    current_detections = []\n",
    "    for box in person_boxes:\n",
    "        # Extract bounding box coordinates, confidence, and center point\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "\n",
    "        current_detections.append((center_x, center_y, x1, y1, x2, y2, confidence))\n",
    "\n",
    "    # Track people manually by comparing current detections with previously tracked people\n",
    "    new_tracked_people = []\n",
    "    for (center_x, center_y, x1, y1, x2, y2, confidence) in current_detections:\n",
    "        matched = False\n",
    "        for person in tracked_people:\n",
    "            prev_id, prev_x, prev_y, prev_x1, prev_y1, prev_x2, prev_y2, prev_conf, last_frame = person\n",
    "            distance = np.sqrt((center_x - prev_x) ** 2 + (center_y - prev_y) ** 2)\n",
    "            if distance < max_distance:\n",
    "                new_tracked_people.append((prev_id, center_x, center_y, x1, y1, x2, y2, confidence, frame_count))\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        \n",
    "        if not matched:\n",
    "            # Assign a new ID for this person\n",
    "            new_tracked_people.append((person_id, center_x, center_y, x1, y1, x2, y2, confidence, frame_count))\n",
    "            person_id += 1\n",
    "\n",
    "    # Update the list of tracked people\n",
    "    tracked_people = new_tracked_people\n",
    "\n",
    "    # Draw the results on the frame\n",
    "    for person in tracked_people:\n",
    "        person_id, center_x, center_y, x1, y1, x2, y2, confidence, _ = person\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display ID and confidence on top of the bounding box\n",
    "        label = f'ID: {person_id} Conf: {confidence:.2f}'\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the output frame\n",
    "    cv2.imshow('Tracked People', frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 25 persons, 1 airplane, 2 backpacks, 5 handbags, 1091.2ms\n",
      "Speed: 3.0ms preprocess, 1091.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 1 backpack, 5 handbags, 1028.2ms\n",
      "Speed: 3.0ms preprocess, 1028.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 airplane, 1 backpack, 3 handbags, 1017.2ms\n",
      "Speed: 2.0ms preprocess, 1017.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 1 backpack, 5 handbags, 1004.2ms\n",
      "Speed: 2.0ms preprocess, 1004.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 1 backpack, 3 handbags, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 6 handbags, 1003.2ms\n",
      "Speed: 3.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 5 handbags, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 3 handbags, 1018.2ms\n",
      "Speed: 2.0ms preprocess, 1018.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 2 handbags, 1007.2ms\n",
      "Speed: 2.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 backpack, 4 handbags, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 backpack, 3 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 backpack, 5 handbags, 1029.2ms\n",
      "Speed: 2.0ms preprocess, 1029.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 4 handbags, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 backpack, 3 handbags, 1001.2ms\n",
      "Speed: 2.0ms preprocess, 1001.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 backpacks, 2 handbags, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 3 backpacks, 4 handbags, 1009.2ms\n",
      "Speed: 3.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 backpacks, 5 handbags, 1003.2ms\n",
      "Speed: 2.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 backpacks, 4 handbags, 1024.2ms\n",
      "Speed: 3.0ms preprocess, 1024.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 2 backpacks, 2 handbags, 1026.2ms\n",
      "Speed: 2.0ms preprocess, 1026.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 airplane, 1 backpack, 3 handbags, 1028.2ms\n",
      "Speed: 2.0ms preprocess, 1028.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 boat, 1 backpack, 1 handbag, 1003.2ms\n",
      "Speed: 3.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 boat, 1 backpack, 1 handbag, 1026.2ms\n",
      "Speed: 3.0ms preprocess, 1026.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1 airplane, 1 boat, 2 backpacks, 2 handbags, 1024.2ms\n",
      "Speed: 3.0ms preprocess, 1024.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 boat, 2 backpacks, 1 handbag, 1008.2ms\n",
      "Speed: 2.0ms preprocess, 1008.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 1 boat, 2 backpacks, 2 handbags, 1031.2ms\n",
      "Speed: 4.0ms preprocess, 1031.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 1 boat, 2 backpacks, 1 handbag, 1003.2ms\n",
      "Speed: 3.0ms preprocess, 1003.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1 airplane, 1 boat, 2 backpacks, 1 handbag, 1 suitcase, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 2 backpacks, 1 handbag, 1000.2ms\n",
      "Speed: 3.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 1 truck, 2 backpacks, 2 handbags, 1017.2ms\n",
      "Speed: 2.0ms preprocess, 1017.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 1 truck, 3 backpacks, 3 handbags, 1027.2ms\n",
      "Speed: 2.0ms preprocess, 1027.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 airplane, 2 backpacks, 2 handbags, 998.2ms\n",
      "Speed: 3.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 2 backpacks, 3 handbags, 1008.2ms\n",
      "Speed: 3.0ms preprocess, 1008.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 airplane, 1 boat, 2 backpacks, 3 handbags, 1000.2ms\n",
      "Speed: 2.0ms preprocess, 1000.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 airplane, 2 backpacks, 2 handbags, 997.2ms\n",
      "Speed: 3.0ms preprocess, 997.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 3 backpacks, 3 handbags, 1 suitcase, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 3 backpacks, 4 handbags, 1024.2ms\n",
      "Speed: 2.0ms preprocess, 1024.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 airplane, 3 backpacks, 4 handbags, 1034.2ms\n",
      "Speed: 2.0ms preprocess, 1034.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 5 backpacks, 2 handbags, 1019.2ms\n",
      "Speed: 3.0ms preprocess, 1019.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1 airplane, 4 backpacks, 4 handbags, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 4 backpacks, 2 handbags, 998.2ms\n",
      "Speed: 2.0ms preprocess, 998.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 airplane, 3 backpacks, 2 handbags, 1 skateboard, 1015.2ms\n",
      "Speed: 2.0ms preprocess, 1015.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 4 backpacks, 3 handbags, 1 skateboard, 1 chair, 1026.2ms\n",
      "Speed: 3.0ms preprocess, 1026.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 2 backpacks, 5 handbags, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 airplane, 3 backpacks, 5 handbags, 1 chair, 1017.2ms\n",
      "Speed: 3.0ms preprocess, 1017.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 airplane, 2 backpacks, 5 handbags, 1 chair, 1024.2ms\n",
      "Speed: 3.0ms preprocess, 1024.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 airplane, 2 backpacks, 5 handbags, 1 chair, 1024.2ms\n",
      "Speed: 3.0ms preprocess, 1024.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 airplane, 2 backpacks, 5 handbags, 1023.2ms\n",
      "Speed: 3.0ms preprocess, 1023.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 3 backpacks, 4 handbags, 1028.2ms\n",
      "Speed: 3.0ms preprocess, 1028.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 airplane, 3 backpacks, 4 handbags, 1 suitcase, 1017.2ms\n",
      "Speed: 3.0ms preprocess, 1017.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 3 backpacks, 4 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 2 backpacks, 4 handbags, 1023.2ms\n",
      "Speed: 3.0ms preprocess, 1023.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 airplane, 2 backpacks, 2 handbags, 1 skateboard, 1021.2ms\n",
      "Speed: 3.0ms preprocess, 1021.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 airplane, 2 backpacks, 1 handbag, 1 skateboard, 999.2ms\n",
      "Speed: 2.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 1 backpack, 2 handbags, 1 skateboard, 1005.2ms\n",
      "Speed: 2.0ms preprocess, 1005.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 airplane, 1 backpack, 3 handbags, 1054.2ms\n",
      "Speed: 3.0ms preprocess, 1054.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 1 backpack, 4 handbags, 1 suitcase, 1029.2ms\n",
      "Speed: 3.0ms preprocess, 1029.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 1 backpack, 4 handbags, 1 suitcase, 1036.2ms\n",
      "Speed: 2.0ms preprocess, 1036.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 1 backpack, 4 handbags, 1 suitcase, 1036.2ms\n",
      "Speed: 3.0ms preprocess, 1036.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 2 backpacks, 4 handbags, 1 suitcase, 1018.2ms\n",
      "Speed: 3.0ms preprocess, 1018.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 2 backpacks, 3 handbags, 1 suitcase, 1013.2ms\n",
      "Speed: 2.0ms preprocess, 1013.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 2 backpacks, 5 handbags, 1002.2ms\n",
      "Speed: 2.0ms preprocess, 1002.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 backpacks, 5 handbags, 1014.2ms\n",
      "Speed: 3.0ms preprocess, 1014.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 backpacks, 6 handbags, 1026.2ms\n",
      "Speed: 3.0ms preprocess, 1026.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 backpacks, 4 handbags, 1007.2ms\n",
      "Speed: 3.0ms preprocess, 1007.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 backpacks, 2 handbags, 1006.2ms\n",
      "Speed: 2.0ms preprocess, 1006.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 backpacks, 2 handbags, 1009.2ms\n",
      "Speed: 2.0ms preprocess, 1009.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 airplane, 1 backpack, 3 handbags, 1047.2ms\n",
      "Speed: 2.0ms preprocess, 1047.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 2 backpacks, 2 handbags, 995.2ms\n",
      "Speed: 3.0ms preprocess, 995.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 airplane, 2 backpacks, 2 handbags, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8m.pt\")  # Path to your YOLOv8 model\n",
    "model.to('cpu') \n",
    "\n",
    "# Initialize video capture\n",
    "video_path = r'C:\\Users\\ahmad\\Downloads\\1338598-hd_1920_1080_30fps.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize variables\n",
    "tracked_people = []  # List to hold currently tracked people (ID, x, y, last_frame)\n",
    "person_id = 0  # Start with ID 0\n",
    "frame_count = 0\n",
    "max_distance = 50  # Max distance to consider matching people (can be tuned)\n",
    "history = []  # History of people who have left the frame\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Detect people using YOLOv8\n",
    "    results = model(frame)  # Results of detections\n",
    "    boxes = results[0].boxes  # Get bounding boxes\n",
    "\n",
    "    # Filter for \"person\" class (class ID 0)\n",
    "    person_boxes = boxes[boxes.cls == 0]\n",
    "\n",
    "    current_detections = []\n",
    "    for box in person_boxes:\n",
    "        # Extract bounding box coordinates, confidence, and center point\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        confidence = box.conf[0]  # Confidence score\n",
    "\n",
    "        current_detections.append((center_x, center_y, x1, y1, x2, y2, confidence))\n",
    "\n",
    "    # Track people manually by comparing current detections with previously tracked people\n",
    "    new_tracked_people = []\n",
    "    for (center_x, center_y, x1, y1, x2, y2, confidence) in current_detections:\n",
    "        matched = False\n",
    "        # Check if this person matches a previously tracked person\n",
    "        for person in tracked_people:\n",
    "            prev_id, prev_x, prev_y, prev_x1, prev_y1, prev_x2, prev_y2, prev_conf, last_frame = person\n",
    "            distance = np.sqrt((center_x - prev_x) ** 2 + (center_y - prev_y) ** 2)\n",
    "            if distance < max_distance:\n",
    "                new_tracked_people.append((prev_id, center_x, center_y, x1, y1, x2, y2, confidence, frame_count))\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        # If no match is found, check the history for a potential returning person\n",
    "        if not matched:\n",
    "            for person in history:\n",
    "                hist_id, hist_x, hist_y, last_seen_frame = person\n",
    "                distance = np.sqrt((center_x - hist_x) ** 2 + (center_y - hist_y) ** 2)\n",
    "                if distance < max_distance:\n",
    "                    # Reassign the historical ID to this returning person\n",
    "                    new_tracked_people.append((hist_id, center_x, center_y, x1, y1, x2, y2, confidence, frame_count))\n",
    "                    history.remove(person)  # Remove from history since they're back in the frame\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "        # If still no match, assign a new ID\n",
    "        if not matched:\n",
    "            new_tracked_people.append((person_id, center_x, center_y, x1, y1, x2, y2, confidence, frame_count))\n",
    "            person_id += 1\n",
    "\n",
    "    # Update the list of tracked people\n",
    "    tracked_people = new_tracked_people\n",
    "\n",
    "    # Move people who are no longer detected into the history\n",
    "    for person in tracked_people:\n",
    "        person_id, center_x, center_y, x1, y1, x2, y2, confidence, last_frame = person\n",
    "        if frame_count - last_frame > 5:  # Adjust this threshold to tune sensitivity for disappearing people\n",
    "            # If the person has been out of frame for some time, add them to the history\n",
    "            history.append((person_id, center_x, center_y, last_frame))\n",
    "            tracked_people.remove(person)\n",
    "\n",
    "    # Draw the results on the frame\n",
    "    for person in tracked_people:\n",
    "        person_id, center_x, center_y, x1, y1, x2, y2, confidence, _ = person\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display ID and confidence on top of the bounding box\n",
    "        label = f'ID: {person_id} Conf: {confidence:.2f}'\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the output frame\n",
    "    cv2.imshow('Tracked People', frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
